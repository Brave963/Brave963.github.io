{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Brave963/Brave963.github.io/blob/main/colab_webui.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ÁéØÂ¢ÉÈÖçÁΩÆ environment"
      ],
      "metadata": {
        "id": "_o6a8GS2lWQM"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9b7iFV3dm1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1ea1ac1-75dd-4c43-90fe-33e4c7f8efb3"
      },
      "source": [
        "!pip install -q condacolab\n",
        "# Setting up condacolab and installing packages\n",
        "import condacolab\n",
        "condacolab.install_from_url(\"https://repo.anaconda.com/miniconda/Miniconda3-py39_23.11.0-2-Linux-x86_64.sh\")\n",
        "%cd -q /content\n",
        "!git clone https://github.com/RVC-Boss/GPT-SoVITS\n",
        "!conda install -y -q -c pytorch -c nvidia cudatoolkit\n",
        "%cd -q /content/GPT-SoVITS\n",
        "!conda install -y -q -c conda-forge gcc gxx ffmpeg cmake -c pytorch -c nvidia\n",
        "!/usr/local/bin/pip install -r requirements.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ú®üç∞‚ú® Everything looks OK!\n",
            "fatal: destination path 'GPT-SoVITS' already exists and is not an empty directory.\n",
            "Error while loading conda entry point: conda-libmamba-solver (libarchive.so.20: cannot open shared object file: No such file or directory)\n",
            "/usr/local/lib/python3.9/site-packages/conda/base/context.py:198: FutureWarning: Adding 'defaults' to channel list implicitly is deprecated and will be removed in 25.3. \n",
            "\n",
            "To remove this warning, please choose a default channel explicitly with conda's regular configuration system, e.g. by adding 'defaults' to the list of channels:\n",
            "\n",
            "  conda config --add channels defaults\n",
            "\n",
            "For more information see https://docs.conda.io/projects/conda/en/stable/user-guide/configuration/use-condarc.html\n",
            "\n",
            "  deprecated.topic(\n",
            "\n",
            "CondaValueError: You have chosen a non-default solver backend (libmamba) but it was not recognized. Choose one of: classic\n",
            "\n",
            "Error while loading conda entry point: conda-libmamba-solver (libarchive.so.20: cannot open shared object file: No such file or directory)\n",
            "/usr/local/lib/python3.9/site-packages/conda/base/context.py:198: FutureWarning: Adding 'defaults' to channel list implicitly is deprecated and will be removed in 25.3. \n",
            "\n",
            "To remove this warning, please choose a default channel explicitly with conda's regular configuration system, e.g. by adding 'defaults' to the list of channels:\n",
            "\n",
            "  conda config --add channels defaults\n",
            "\n",
            "For more information see https://docs.conda.io/projects/conda/en/stable/user-guide/configuration/use-condarc.html\n",
            "\n",
            "  deprecated.topic(\n",
            "\n",
            "CondaValueError: You have chosen a non-default solver backend (libmamba) but it was not recognized. Choose one of: classic\n",
            "\n",
            "Ignoring onnxruntime: markers 'sys_platform == \"darwin\"' don't match your environment\n",
            "Ignoring opencc: markers 'sys_platform != \"linux\"' don't match your environment\n",
            "Requirement already satisfied: numpy==1.23.4 in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (1.23.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (1.13.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (2.18.0)\n",
            "Requirement already satisfied: librosa==0.9.2 in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (0.9.2)\n",
            "Requirement already satisfied: numba==0.56.4 in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 5)) (0.56.4)\n",
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 6)) (2.4.0)\n",
            "Requirement already satisfied: gradio<=4.24.0,>=4.0 in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 7)) (4.24.0)\n",
            "Requirement already satisfied: ffmpeg-python in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 8)) (0.2.0)\n",
            "Requirement already satisfied: onnxruntime-gpu in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 10)) (1.19.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 11)) (4.66.5)\n",
            "Requirement already satisfied: funasr==1.0.27 in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 12)) (1.0.27)\n",
            "Requirement already satisfied: cn2an in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 13)) (0.5.22)\n",
            "Requirement already satisfied: pypinyin in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 14)) (0.53.0)\n",
            "Requirement already satisfied: pyopenjtalk>=0.3.4 in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 15)) (0.3.4)\n",
            "Requirement already satisfied: g2p_en in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 16)) (2.1.0)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 17)) (2.5.0)\n",
            "Requirement already satisfied: modelscope==1.10.0 in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 18)) (1.10.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 19)) (0.2.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 20)) (4.46.0)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 21)) (5.2.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 22)) (6.0.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 23)) (6.1.0)\n",
            "Requirement already satisfied: jieba_fast in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 24)) (0.53)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 25)) (0.42.1)\n",
            "Requirement already satisfied: LangSegment>=0.2.0 in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 26)) (0.3.5)\n",
            "Requirement already satisfied: Faster_Whisper in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 27)) (1.0.3)\n",
            "Requirement already satisfied: wordsegment in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 28)) (1.3.1)\n",
            "Requirement already satisfied: rotary_embedding_torch in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 29)) (0.8.4)\n",
            "Requirement already satisfied: pyjyutping in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 30)) (1.0.0)\n",
            "Requirement already satisfied: g2pk2 in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 31)) (0.0.3)\n",
            "Requirement already satisfied: ko_pron in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 32)) (1.3)\n",
            "Requirement already satisfied: opencc==1.1.1 in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 34)) (1.1.1)\n",
            "Requirement already satisfied: python_mecab_ko in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 35)) (1.3.7)\n",
            "Requirement already satisfied: fastapi<0.112.2 in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 36)) (0.112.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.9/site-packages (from librosa==0.9.2->-r requirements.txt (line 4)) (3.0.1)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.9/site-packages (from librosa==0.9.2->-r requirements.txt (line 4)) (1.5.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.9/site-packages (from librosa==0.9.2->-r requirements.txt (line 4)) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.9/site-packages (from librosa==0.9.2->-r requirements.txt (line 4)) (5.1.1)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.9/site-packages (from librosa==0.9.2->-r requirements.txt (line 4)) (0.4.3)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.9/site-packages (from librosa==0.9.2->-r requirements.txt (line 4)) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.9/site-packages (from librosa==0.9.2->-r requirements.txt (line 4)) (1.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/site-packages (from librosa==0.9.2->-r requirements.txt (line 4)) (23.1)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.9/site-packages (from numba==0.56.4->-r requirements.txt (line 5)) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/site-packages (from numba==0.56.4->-r requirements.txt (line 5)) (68.2.2)\n",
            "Requirement already satisfied: jamo in /usr/local/lib/python3.9/site-packages (from funasr==1.0.27->-r requirements.txt (line 12)) (0.4.1)\n",
            "Requirement already satisfied: kaldiio>=2.17.0 in /usr/local/lib/python3.9/site-packages (from funasr==1.0.27->-r requirements.txt (line 12)) (2.18.0)\n",
            "Requirement already satisfied: torch-complex in /usr/local/lib/python3.9/site-packages (from funasr==1.0.27->-r requirements.txt (line 12)) (0.4.4)\n",
            "Requirement already satisfied: pytorch-wpe in /usr/local/lib/python3.9/site-packages (from funasr==1.0.27->-r requirements.txt (line 12)) (0.0.1)\n",
            "Requirement already satisfied: editdistance>=0.5.2 in /usr/local/lib/python3.9/site-packages (from funasr==1.0.27->-r requirements.txt (line 12)) (0.8.1)\n",
            "Requirement already satisfied: oss2 in /usr/local/lib/python3.9/site-packages (from funasr==1.0.27->-r requirements.txt (line 12)) (2.19.0)\n",
            "Requirement already satisfied: umap-learn in /usr/local/lib/python3.9/site-packages (from funasr==1.0.27->-r requirements.txt (line 12)) (0.5.6)\n",
            "Requirement already satisfied: jaconv in /usr/local/lib/python3.9/site-packages (from funasr==1.0.27->-r requirements.txt (line 12)) (0.4.0)\n",
            "Requirement already satisfied: hydra-core>=1.3.2 in /usr/local/lib/python3.9/site-packages (from funasr==1.0.27->-r requirements.txt (line 12)) (1.3.2)\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.9/site-packages (from funasr==1.0.27->-r requirements.txt (line 12)) (2.6.2.2)\n",
            "Requirement already satisfied: openai-whisper in /usr/local/lib/python3.9/site-packages (from funasr==1.0.27->-r requirements.txt (line 12)) (20240930)\n",
            "Requirement already satisfied: addict in /usr/local/lib/python3.9/site-packages (from modelscope==1.10.0->-r requirements.txt (line 18)) (2.4.0)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.9/site-packages (from modelscope==1.10.0->-r requirements.txt (line 18)) (24.2.0)\n",
            "Requirement already satisfied: datasets>=2.14.5 in /usr/local/lib/python3.9/site-packages (from modelscope==1.10.0->-r requirements.txt (line 18)) (3.0.2)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.9/site-packages (from modelscope==1.10.0->-r requirements.txt (line 18)) (0.8.0)\n",
            "Requirement already satisfied: filelock>=3.3.0 in /usr/local/lib/python3.9/site-packages (from modelscope==1.10.0->-r requirements.txt (line 18)) (3.16.1)\n",
            "Requirement already satisfied: gast>=0.2.2 in /usr/local/lib/python3.9/site-packages (from modelscope==1.10.0->-r requirements.txt (line 18)) (0.6.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/site-packages (from modelscope==1.10.0->-r requirements.txt (line 18)) (2.2.3)\n",
            "Requirement already satisfied: Pillow>=6.2.0 in /usr/local/lib/python3.9/site-packages (from modelscope==1.10.0->-r requirements.txt (line 18)) (10.4.0)\n",
            "Requirement already satisfied: pyarrow!=9.0.0,>=6.0.0 in /usr/local/lib/python3.9/site-packages (from modelscope==1.10.0->-r requirements.txt (line 18)) (17.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.9/site-packages (from modelscope==1.10.0->-r requirements.txt (line 18)) (2.9.0.post0)\n",
            "Requirement already satisfied: requests>=2.25 in /usr/local/lib/python3.9/site-packages (from modelscope==1.10.0->-r requirements.txt (line 18)) (2.32.3)\n",
            "Requirement already satisfied: simplejson>=3.3.0 in /usr/local/lib/python3.9/site-packages (from modelscope==1.10.0->-r requirements.txt (line 18)) (3.19.3)\n",
            "Requirement already satisfied: sortedcontainers>=1.5.9 in /usr/local/lib/python3.9/site-packages (from modelscope==1.10.0->-r requirements.txt (line 18)) (2.4.0)\n",
            "Requirement already satisfied: urllib3>=1.26 in /usr/local/lib/python3.9/site-packages (from modelscope==1.10.0->-r requirements.txt (line 18)) (1.26.18)\n",
            "Requirement already satisfied: yapf in /usr/local/lib/python3.9/site-packages (from modelscope==1.10.0->-r requirements.txt (line 18)) (0.40.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 3)) (2.1.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 3)) (1.67.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 3)) (3.7)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 3)) (5.28.3)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 3)) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 3)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.9/site-packages (from pytorch-lightning->-r requirements.txt (line 6)) (2.5.0)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.9/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning->-r requirements.txt (line 6)) (2024.9.0)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.9/site-packages (from pytorch-lightning->-r requirements.txt (line 6)) (1.5.1)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.9/site-packages (from pytorch-lightning->-r requirements.txt (line 6)) (4.12.2)\n",
            "Requirement already satisfied: lightning-utilities>=0.10.0 in /usr/local/lib/python3.9/site-packages (from pytorch-lightning->-r requirements.txt (line 6)) (0.11.8)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.9/site-packages (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (23.2.1)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.9/site-packages (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (5.4.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.9/site-packages (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (0.4.0)\n",
            "Requirement already satisfied: gradio-client==0.14.0 in /usr/local/lib/python3.9/site-packages (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (0.14.0)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.9/site-packages (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (0.27.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.9/site-packages (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (0.26.1)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.9/site-packages (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (6.4.5)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.9/site-packages (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.9/site-packages (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.9/site-packages (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (3.9.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.9/site-packages (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (3.10.10)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.9/site-packages (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (2.9.2)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.9/site-packages (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.9/site-packages (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (0.0.12)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.9/site-packages (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (0.7.0)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.9/site-packages (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.9/site-packages (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.9 in /usr/local/lib/python3.9/site-packages (from typer[all]<1.0,>=0.9; sys_platform != \"emscripten\"->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (0.12.5)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.9/site-packages (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (0.32.0)\n",
            "Requirement already satisfied: websockets<12.0,>=10.0 in /usr/local/lib/python3.9/site-packages (from gradio-client==0.14.0->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (11.0.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.9/site-packages (from ffmpeg-python->-r requirements.txt (line 8)) (1.0.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.9/site-packages (from onnxruntime-gpu->-r requirements.txt (line 10)) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.9/site-packages (from onnxruntime-gpu->-r requirements.txt (line 10)) (24.3.25)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/site-packages (from onnxruntime-gpu->-r requirements.txt (line 10)) (1.13.1)\n",
            "Requirement already satisfied: proces>=0.1.3 in /usr/local/lib/python3.9/site-packages (from cn2an->-r requirements.txt (line 13)) (0.1.7)\n",
            "Requirement already satisfied: nltk>=3.2.4 in /usr/local/lib/python3.9/site-packages (from g2p_en->-r requirements.txt (line 16)) (3.9.1)\n",
            "Requirement already satisfied: inflect>=0.3.1 in /usr/local/lib/python3.9/site-packages (from g2p_en->-r requirements.txt (line 16)) (7.4.0)\n",
            "Requirement already satisfied: distance>=0.1.3 in /usr/local/lib/python3.9/site-packages (from g2p_en->-r requirements.txt (line 16)) (0.1.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/site-packages (from torch>=2.1.0->pytorch-lightning->-r requirements.txt (line 6)) (3.2.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.9/site-packages (from torch>=2.1.0->pytorch-lightning->-r requirements.txt (line 6)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.9/site-packages (from torch>=2.1.0->pytorch-lightning->-r requirements.txt (line 6)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.9/site-packages (from torch>=2.1.0->pytorch-lightning->-r requirements.txt (line 6)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.9/site-packages (from torch>=2.1.0->pytorch-lightning->-r requirements.txt (line 6)) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.9/site-packages (from torch>=2.1.0->pytorch-lightning->-r requirements.txt (line 6)) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.9/site-packages (from torch>=2.1.0->pytorch-lightning->-r requirements.txt (line 6)) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.9/site-packages (from torch>=2.1.0->pytorch-lightning->-r requirements.txt (line 6)) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.9/site-packages (from torch>=2.1.0->pytorch-lightning->-r requirements.txt (line 6)) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.9/site-packages (from torch>=2.1.0->pytorch-lightning->-r requirements.txt (line 6)) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.9/site-packages (from torch>=2.1.0->pytorch-lightning->-r requirements.txt (line 6)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.9/site-packages (from torch>=2.1.0->pytorch-lightning->-r requirements.txt (line 6)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.9/site-packages (from torch>=2.1.0->pytorch-lightning->-r requirements.txt (line 6)) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.9/site-packages (from torch>=2.1.0->pytorch-lightning->-r requirements.txt (line 6)) (3.1.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.9/site-packages (from sympy->onnxruntime-gpu->-r requirements.txt (line 10)) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/site-packages (from transformers->-r requirements.txt (line 20)) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.9/site-packages (from transformers->-r requirements.txt (line 20)) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.9/site-packages (from transformers->-r requirements.txt (line 20)) (0.20.1)\n",
            "Requirement already satisfied: py3langid>=0.2.2 in /usr/local/lib/python3.9/site-packages (from LangSegment>=0.2.0->-r requirements.txt (line 26)) (0.2.2)\n",
            "Requirement already satisfied: av<13,>=11.0 in /usr/local/lib/python3.9/site-packages (from Faster_Whisper->-r requirements.txt (line 27)) (12.3.0)\n",
            "Requirement already satisfied: ctranslate2<5,>=4.0 in /usr/local/lib/python3.9/site-packages (from Faster_Whisper->-r requirements.txt (line 27)) (4.5.0)\n",
            "Requirement already satisfied: onnxruntime<2,>=1.14 in /usr/local/lib/python3.9/site-packages (from Faster_Whisper->-r requirements.txt (line 27)) (1.19.2)\n",
            "Requirement already satisfied: python-mecab-ko-dic in /usr/local/lib/python3.9/site-packages (from python_mecab_ko->-r requirements.txt (line 35)) (2.1.1.post2)\n",
            "Requirement already satisfied: starlette<0.39.0,>=0.37.2 in /usr/local/lib/python3.9/site-packages (from fastapi<0.112.2->-r requirements.txt (line 36)) (0.38.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.9/site-packages (from altair<6.0,>=4.2.0->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.5.2 in /usr/local/lib/python3.9/site-packages (from altair<6.0,>=4.2.0->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (1.10.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.9/site-packages (from datasets>=2.14.5->modelscope==1.10.0->-r requirements.txt (line 18)) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/site-packages (from datasets>=2.14.5->modelscope==1.10.0->-r requirements.txt (line 18)) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.9/site-packages (from datasets>=2.14.5->modelscope==1.10.0->-r requirements.txt (line 18)) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/site-packages (from datasets>=2.14.5->modelscope==1.10.0->-r requirements.txt (line 18)) (3.10.10)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.9/site-packages (from httpx>=0.24.1->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (4.6.2.post1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.9/site-packages (from httpx>=0.24.1->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.9/site-packages (from httpx>=0.24.1->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (1.0.6)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.9/site-packages (from httpx>=0.24.1->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (3.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.9/site-packages (from httpx>=0.24.1->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.9/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (0.14.0)\n",
            "Requirement already satisfied: omegaconf<2.4,>=2.2 in /usr/local/lib/python3.9/site-packages (from hydra-core>=1.3.2->funasr==1.0.27->-r requirements.txt (line 12)) (2.3.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.9/site-packages (from hydra-core>=1.3.2->funasr==1.0.27->-r requirements.txt (line 12)) (4.9.3)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/site-packages (from importlib-resources<7.0,>=1.3->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (3.20.2)\n",
            "Requirement already satisfied: more-itertools>=8.5.0 in /usr/local/lib/python3.9/site-packages (from inflect>=0.3.1->g2p_en->-r requirements.txt (line 16)) (10.5.0)\n",
            "Requirement already satisfied: typeguard>=4.0.1 in /usr/local/lib/python3.9/site-packages (from inflect>=0.3.1->g2p_en->-r requirements.txt (line 16)) (4.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard->-r requirements.txt (line 3)) (8.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/site-packages (from matplotlib~=3.0->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/site-packages (from matplotlib~=3.0->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/site-packages (from matplotlib~=3.0->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.9/site-packages (from matplotlib~=3.0->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/site-packages (from matplotlib~=3.0->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (3.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/site-packages (from nltk>=3.2.4->g2p_en->-r requirements.txt (line 16)) (8.1.7)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/site-packages (from pandas->modelscope==1.10.0->-r requirements.txt (line 18)) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.9/site-packages (from pandas->modelscope==1.10.0->-r requirements.txt (line 18)) (2024.2)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.9/site-packages (from pooch>=1.0->librosa==0.9.2->-r requirements.txt (line 4)) (3.10.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.9/site-packages (from pydantic>=2.0->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.9/site-packages (from pydantic>=2.0->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/site-packages (from requests>=2.25->modelscope==1.10.0->-r requirements.txt (line 18)) (2.0.4)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.9/site-packages (from scikit-learn>=0.19.1->librosa==0.9.2->-r requirements.txt (line 4)) (3.5.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.9/site-packages (from soundfile>=0.10.2->librosa==0.9.2->-r requirements.txt (line 4)) (1.16.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.9/site-packages (from typer<1.0,>=0.9->typer[all]<1.0,>=0.9; sys_platform != \"emscripten\"->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.9/site-packages (from typer<1.0,>=0.9->typer[all]<1.0,>=0.9; sys_platform != \"emscripten\"->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (13.9.3)\n",
            "\u001b[33mWARNING: typer 0.12.5 does not provide the extra 'all'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.9/site-packages (from coloredlogs->onnxruntime-gpu->-r requirements.txt (line 10)) (10.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.9/site-packages (from openai-whisper->funasr==1.0.27->-r requirements.txt (line 12)) (0.8.0)\n",
            "Requirement already satisfied: crcmod>=1.7 in /usr/local/lib/python3.9/site-packages (from oss2->funasr==1.0.27->-r requirements.txt (line 12)) (1.7)\n",
            "Requirement already satisfied: pycryptodome>=3.4.7 in /usr/local/lib/python3.9/site-packages (from oss2->funasr==1.0.27->-r requirements.txt (line 12)) (3.21.0)\n",
            "Requirement already satisfied: aliyun-python-sdk-kms>=2.4.1 in /usr/local/lib/python3.9/site-packages (from oss2->funasr==1.0.27->-r requirements.txt (line 12)) (2.16.5)\n",
            "Requirement already satisfied: aliyun-python-sdk-core>=2.13.12 in /usr/local/lib/python3.9/site-packages (from oss2->funasr==1.0.27->-r requirements.txt (line 12)) (2.16.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.9/site-packages (from umap-learn->funasr==1.0.27->-r requirements.txt (line 12)) (0.5.13)\n",
            "Requirement already satisfied: tomli>=2.0.1 in /usr/local/lib/python3.9/site-packages (from yapf->modelscope==1.10.0->-r requirements.txt (line 18)) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.9/site-packages (from aiohttp->datasets>=2.14.5->modelscope==1.10.0->-r requirements.txt (line 18)) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/site-packages (from aiohttp->datasets>=2.14.5->modelscope==1.10.0->-r requirements.txt (line 18)) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/site-packages (from aiohttp->datasets>=2.14.5->modelscope==1.10.0->-r requirements.txt (line 18)) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/site-packages (from aiohttp->datasets>=2.14.5->modelscope==1.10.0->-r requirements.txt (line 18)) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.9/site-packages (from aiohttp->datasets>=2.14.5->modelscope==1.10.0->-r requirements.txt (line 18)) (1.16.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.9/site-packages (from aiohttp->datasets>=2.14.5->modelscope==1.10.0->-r requirements.txt (line 18)) (4.0.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.9.3 in /usr/local/lib/python3.9/site-packages (from aliyun-python-sdk-core>=2.13.12->oss2->funasr==1.0.27->-r requirements.txt (line 12)) (0.10.0)\n",
            "Requirement already satisfied: cryptography>=3.0.0 in /usr/local/lib/python3.9/site-packages (from aliyun-python-sdk-core>=2.13.12->oss2->funasr==1.0.27->-r requirements.txt (line 12)) (41.0.7)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.9/site-packages (from anyio->httpx>=0.24.1->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (1.2.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/site-packages (from cffi>=1.0->soundfile>=0.10.2->librosa==0.9.2->-r requirements.txt (line 4)) (2.21)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.9/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.9/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.9/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (0.20.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.9/site-packages (from rich>=10.11.0->typer<1.0,>=0.9->typer[all]<1.0,>=0.9; sys_platform != \"emscripten\"->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/site-packages (from rich>=10.11.0->typer<1.0,>=0.9->typer[all]<1.0,>=0.9; sys_platform != \"emscripten\"->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.9->typer[all]<1.0,>=0.9; sys_platform != \"emscripten\"->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (0.1.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.9/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets>=2.14.5->modelscope==1.10.0->-r requirements.txt (line 18)) (0.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download pretrained models ‰∏ãËΩΩÈ¢ÑËÆ≠ÁªÉÊ®°Âûã\n",
        "!mkdir -p /content/GPT-SoVITS/GPT_SoVITS/pretrained_models\n",
        "!mkdir -p /content/GPT-SoVITS/tools/damo_asr/models\n",
        "!mkdir -p /content/GPT-SoVITS/tools/uvr5\n",
        "%cd /content/GPT-SoVITS/GPT_SoVITS/pretrained_models\n",
        "!git clone https://huggingface.co/lj1995/GPT-SoVITS\n",
        "%cd /content/GPT-SoVITS/tools/damo_asr/models\n",
        "!git clone https://www.modelscope.cn/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch.git\n",
        "!git clone https://www.modelscope.cn/damo/speech_fsmn_vad_zh-cn-16k-common-pytorch.git\n",
        "!git clone https://www.modelscope.cn/damo/punc_ct-transformer_zh-cn-common-vocab272727-pytorch.git\n",
        "# @title UVR5 pretrains ÂÆâË£Öuvr5Ê®°Âûã\n",
        "%cd /content/GPT-SoVITS/tools/uvr5\n",
        "%rm -r uvr5_weights\n",
        "!git clone https://huggingface.co/Delik/uvr5_weights\n",
        "!git config core.sparseCheckout true\n",
        "!mv /content/GPT-SoVITS/GPT_SoVITS/pretrained_models/GPT-SoVITS/* /content/GPT-SoVITS/GPT_SoVITS/pretrained_models/"
      ],
      "metadata": {
        "id": "0NgxXg5sjv7z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80710540-01f5-4c05-aa07-53152b6d9f0c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/GPT-SoVITS/GPT_SoVITS/pretrained_models\n",
            "Cloning into 'GPT-SoVITS'...\n",
            "remote: Enumerating objects: 29, done.\u001b[K\n",
            "remote: Counting objects: 100% (25/25), done.\u001b[K\n",
            "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
            "remote: Total 29 (delta 3), reused 0 (delta 0), pack-reused 4 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (29/29), 104.78 KiB | 3.61 MiB/s, done.\n",
            "Filtering content: 100% (8/8), 1.44 GiB | 48.43 MiB/s, done.\n",
            "/content/GPT-SoVITS/tools/damo_asr/models\n",
            "Cloning into 'speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch'...\n",
            "remote: Enumerating objects: 466, done.\u001b[K\n",
            "remote: Counting objects: 100% (68/68), done.\u001b[K\n",
            "remote: Compressing objects: 100% (38/38), done.\u001b[K\n",
            "remote: Total 466 (delta 38), reused 57 (delta 30), pack-reused 398\u001b[K\n",
            "Receiving objects: 100% (466/466), 1.12 GiB | 12.19 MiB/s, done.\n",
            "Resolving deltas: 100% (268/268), done.\n",
            "Cloning into 'speech_fsmn_vad_zh-cn-16k-common-pytorch'...\n",
            "remote: Enumerating objects: 184, done.\u001b[K\n",
            "remote: Counting objects: 100% (39/39), done.\u001b[K\n",
            "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
            "remote: Total 184 (delta 18), reused 23 (delta 10), pack-reused 145\u001b[K\n",
            "Receiving objects: 100% (184/184), 4.66 MiB | 20.56 MiB/s, done.\n",
            "Resolving deltas: 100% (91/91), done.\n",
            "Cloning into 'punc_ct-transformer_zh-cn-common-vocab272727-pytorch'...\n",
            "remote: Enumerating objects: 170, done.\u001b[K\n",
            "remote: Counting objects: 100% (50/50), done.\u001b[K\n",
            "remote: Compressing objects: 100% (36/36), done.\u001b[K\n",
            "remote: Total 170 (delta 25), reused 30 (delta 14), pack-reused 120\u001b[K\n",
            "Receiving objects: 100% (170/170), 257.56 MiB | 32.82 MiB/s, done.\n",
            "Resolving deltas: 100% (86/86), done.\n",
            "/content/GPT-SoVITS/tools/uvr5\n",
            "Cloning into 'uvr5_weights'...\n",
            "remote: Enumerating objects: 15, done.\u001b[K\n",
            "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 15 (delta 1), reused 0 (delta 0), pack-reused 3 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (15/15), 3.25 KiB | 1.08 MiB/s, done.\n",
            "Filtering content: 100% (9/9), 594.44 MiB | 69.07 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title launch WebUI ÂêØÂä®WebUI\n",
        "!/usr/local/bin/pip install ipykernel\n",
        "!sed -i '10s/False/True/' /content/GPT-SoVITS/config.py\n",
        "%cd /content/GPT-SoVITS/\n",
        "!/usr/local/bin/python  webui.py"
      ],
      "metadata": {
        "id": "4oRGUzkrk8C7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e0b3944-0e6b-43c2-d0fb-f4521446369f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ipykernel\n",
            "  Downloading ipykernel-6.29.5-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting comm>=0.1.1 (from ipykernel)\n",
            "  Downloading comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting debugpy>=1.6.5 (from ipykernel)\n",
            "  Downloading debugpy-1.8.7-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting ipython>=7.23.1 (from ipykernel)\n",
            "  Downloading ipython-8.18.1-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting jupyter-client>=6.1.12 (from ipykernel)\n",
            "  Downloading jupyter_client-8.6.3-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting jupyter-core!=5.0.*,>=4.12 (from ipykernel)\n",
            "  Downloading jupyter_core-5.7.2-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting matplotlib-inline>=0.1 (from ipykernel)\n",
            "  Downloading matplotlib_inline-0.1.7-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting nest-asyncio (from ipykernel)\n",
            "  Downloading nest_asyncio-1.6.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/site-packages (from ipykernel) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.9/site-packages (from ipykernel) (6.1.0)\n",
            "Collecting pyzmq>=24 (from ipykernel)\n",
            "  Downloading pyzmq-26.2.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.2 kB)\n",
            "Collecting tornado>=6.1 (from ipykernel)\n",
            "  Downloading tornado-6.4.1-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
            "Collecting traitlets>=5.4.0 (from ipykernel)\n",
            "  Downloading traitlets-5.14.3-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (5.1.1)\n",
            "Collecting jedi>=0.16 (from ipython>=7.23.1->ipykernel)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting prompt-toolkit<3.1.0,>=3.0.41 (from ipython>=7.23.1->ipykernel)\n",
            "  Downloading prompt_toolkit-3.0.48-py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (2.18.0)\n",
            "Collecting stack-data (from ipython>=7.23.1->ipykernel)\n",
            "  Downloading stack_data-0.6.3-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (4.12.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (1.2.2)\n",
            "Collecting pexpect>4.3 (from ipython>=7.23.1->ipykernel)\n",
            "  Downloading pexpect-4.9.0-py2.py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.3 in /usr/local/lib/python3.9/site-packages (from jupyter-client>=6.1.12->ipykernel) (8.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.9/site-packages (from jupyter-client>=6.1.12->ipykernel) (2.9.0.post0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.9/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (3.10.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.9/site-packages (from importlib-metadata>=4.8.3->jupyter-client>=6.1.12->ipykernel) (3.20.2)\n",
            "Collecting parso<0.9.0,>=0.8.3 (from jedi>=0.16->ipython>=7.23.1->ipykernel)\n",
            "  Downloading parso-0.8.4-py2.py3-none-any.whl.metadata (7.7 kB)\n",
            "Collecting ptyprocess>=0.5 (from pexpect>4.3->ipython>=7.23.1->ipykernel)\n",
            "  Downloading ptyprocess-0.7.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting wcwidth (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel)\n",
            "  Downloading wcwidth-0.2.13-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel) (1.16.0)\n",
            "Collecting executing>=1.2.0 (from stack-data->ipython>=7.23.1->ipykernel)\n",
            "  Downloading executing-2.1.0-py2.py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting asttokens>=2.1.0 (from stack-data->ipython>=7.23.1->ipykernel)\n",
            "  Downloading asttokens-2.4.1-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting pure-eval (from stack-data->ipython>=7.23.1->ipykernel)\n",
            "  Downloading pure_eval-0.2.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Downloading ipykernel-6.29.5-py3-none-any.whl (117 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m117.2/117.2 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n",
            "Downloading debugpy-1.8.7-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ipython-8.18.1-py3-none-any.whl (808 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m808.2/808.2 kB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_client-8.6.3-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m106.1/106.1 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_core-5.7.2-py3-none-any.whl (28 kB)\n",
            "Downloading matplotlib_inline-0.1.7-py3-none-any.whl (9.9 kB)\n",
            "Downloading pyzmq-26.2.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (862 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m862.1/862.1 kB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tornado-6.4.1-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (436 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m436.8/436.8 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading traitlets-5.14.3-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nest_asyncio-1.6.0-py3-none-any.whl (5.2 kB)\n",
            "Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pexpect-4.9.0-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prompt_toolkit-3.0.48-py3-none-any.whl (386 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m386.6/386.6 kB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stack_data-0.6.3-py3-none-any.whl (24 kB)\n",
            "Downloading asttokens-2.4.1-py2.py3-none-any.whl (27 kB)\n",
            "Downloading executing-2.1.0-py2.py3-none-any.whl (25 kB)\n",
            "Downloading parso-0.8.4-py2.py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m103.7/103.7 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)\n",
            "Downloading pure_eval-0.2.3-py3-none-any.whl (11 kB)\n",
            "Downloading wcwidth-0.2.13-py2.py3-none-any.whl (34 kB)\n",
            "Installing collected packages: wcwidth, pure-eval, ptyprocess, traitlets, tornado, pyzmq, prompt-toolkit, pexpect, parso, nest-asyncio, executing, debugpy, asttokens, stack-data, matplotlib-inline, jupyter-core, jedi, comm, jupyter-client, ipython, ipykernel\n",
            "Successfully installed asttokens-2.4.1 comm-0.2.2 debugpy-1.8.7 executing-2.1.0 ipykernel-6.29.5 ipython-8.18.1 jedi-0.19.1 jupyter-client-8.6.3 jupyter-core-5.7.2 matplotlib-inline-0.1.7 nest-asyncio-1.6.0 parso-0.8.4 pexpect-4.9.0 prompt-toolkit-3.0.48 ptyprocess-0.7.0 pure-eval-0.2.3 pyzmq-26.2.0 stack-data-0.6.3 tornado-6.4.1 traitlets-5.14.3 wcwidth-0.2.13\n",
            "/content/GPT-SoVITS\n",
            "Downloading g2pw model...\n",
            "Extracting g2pw model...\n",
            "Running on local URL:  http://0.0.0.0:9874\n",
            "Running on public URL: https://2fa947f336565abf36.gradio.live\n",
            "\"/usr/local/bin/python\" tools/uvr5/webui.py \"cuda\" True 9873 True\n",
            "Running on local URL:  http://0.0.0.0:9873\n",
            "Running on public URL: https://73312e631f0e525d41.gradio.live\n",
            "\"/usr/local/bin/python\" tools/slice_audio.py \"/content/GPT-SoVITS/input\" \"output/slicer_opt\" -34 4000 300 5 500 0.95 0.25 0 4\n",
            "\"/usr/local/bin/python\" tools/slice_audio.py \"/content/GPT-SoVITS/input\" \"output/slicer_opt\" -34 4000 300 5 500 0.95 0.25 1 4\n",
            "\"/usr/local/bin/python\" tools/slice_audio.py \"/content/GPT-SoVITS/input\" \"output/slicer_opt\" -34 4000 300 5 500 0.95 0.25 2 4\n",
            "\"/usr/local/bin/python\" tools/slice_audio.py \"/content/GPT-SoVITS/input\" \"output/slicer_opt\" -34 4000 300 5 500 0.95 0.25 3 4\n",
            "ÊâßË°åÂÆåÊØïÔºåËØ∑Ê£ÄÊü•ËæìÂá∫Êñá‰ª∂\n",
            "ÊâßË°åÂÆåÊØïÔºåËØ∑Ê£ÄÊü•ËæìÂá∫Êñá‰ª∂\n",
            "ÊâßË°åÂÆåÊØïÔºåËØ∑Ê£ÄÊü•ËæìÂá∫Êñá‰ª∂\n",
            "ÊâßË°åÂÆåÊØïÔºåËØ∑Ê£ÄÊü•ËæìÂá∫Êñá‰ª∂\n",
            "\"/usr/local/bin/python\" tools/cmd-denoise.py -i \"output/slicer_opt\" -o \"output/denoise_opt\" -p float16\n",
            "2024-10-24 12:37:56,272 - modelscope - INFO - PyTorch version 2.5.0 Found.\n",
            "2024-10-24 12:37:56,273 - modelscope - INFO - Loading ast index from /root/.cache/modelscope/ast_indexer\n",
            "2024-10-24 12:37:56,273 - modelscope - INFO - No valid ast index found from /root/.cache/modelscope/ast_indexer, generating ast index from prebuilt!\n",
            "2024-10-24 12:37:56,333 - modelscope - INFO - Loading done! Current index file version is 1.10.0, with md5 6f80e611660fd927919f53be0f3352ab and a total number of 946 components indexed\n",
            "2024-10-24 12:38:06,526 - modelscope - WARNING - Model revision not specified, use revision: v1.0.2\n",
            "Downloading: 100% 1.45k/1.45k [00:00<00:00, 147kB/s]\n",
            "Downloading: 100% 903/903 [00:00<00:00, 91.9kB/s]\n",
            "Downloading: 100% 177k/177k [00:00<00:00, 2.88MB/s]\n",
            "Downloading: 100% 88.2k/88.2k [00:00<00:00, 1.66MB/s]\n",
            "Downloading: 100% 55.3M/55.3M [00:00<00:00, 130MB/s]\n",
            "Downloading: 100% 12.8k/12.8k [00:00<00:00, 1.33MB/s]\n",
            "Downloading: 100% 75.0k/75.0k [00:00<00:00, 1.41MB/s]\n",
            "Downloading: 100% 152k/152k [00:00<00:00, 2.96MB/s]\n",
            "2024-10-24 12:38:16,449 - modelscope - INFO - initiate model from /root/.cache/modelscope/hub/damo/speech_frcrn_ans_cirm_16k\n",
            "2024-10-24 12:38:16,449 - modelscope - INFO - initiate model from location /root/.cache/modelscope/hub/damo/speech_frcrn_ans_cirm_16k.\n",
            "2024-10-24 12:38:16,451 - modelscope - INFO - initialize model from /root/.cache/modelscope/hub/damo/speech_frcrn_ans_cirm_16k\n",
            "/usr/local/lib/python3.9/site-packages/modelscope/models/audio/ans/frcrn.py:35: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(\n",
            "2024-10-24 12:38:17,537 - modelscope - WARNING - No preprocessor field found in cfg.\n",
            "2024-10-24 12:38:17,539 - modelscope - WARNING - No val key and type key found in preprocessor domain of configuration.json file.\n",
            "2024-10-24 12:38:17,539 - modelscope - WARNING - Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': '/root/.cache/modelscope/hub/damo/speech_frcrn_ans_cirm_16k'}. trying to build by task and model information.\n",
            "2024-10-24 12:38:17,540 - modelscope - WARNING - No preprocessor key ('speech_frcrn_ans_cirm_16k', 'acoustic-noise-suppression') found in PREPROCESSOR_MAP, skip building preprocessor.\n",
            "  0% 0/6 [00:00<?, ?it/s]inputs:(1, 62800)\n",
            "padding: 26800\n",
            "inputs after padding:(1, 89600)\n",
            " 17% 1/6 [00:02<00:14,  2.87s/it]inputs:(1, 62960)\n",
            "padding: 26960\n",
            "inputs after padding:(1, 89920)\n",
            " 33% 2/6 [00:03<00:05,  1.43s/it]inputs:(1, 87280)\n",
            "padding: 27280\n",
            "inputs after padding:(1, 114560)\n",
            " 50% 3/6 [00:03<00:03,  1.06s/it]inputs:(1, 64960)\n",
            "padding: 16960\n",
            "inputs after padding:(1, 81920)\n",
            " 67% 4/6 [00:04<00:01,  1.24it/s]inputs:(1, 109760)\n",
            "padding: 25760\n",
            "inputs after padding:(1, 135520)\n",
            " 83% 5/6 [00:05<00:00,  1.29it/s]inputs:(1, 93200)\n",
            "padding: 21200\n",
            "inputs after padding:(1, 114400)\n",
            "100% 6/6 [00:05<00:00,  1.06it/s]\n",
            "\"/usr/local/bin/python\" tools/asr/funasr_asr.py -i \"output/denoise_opt\" -o \"output/asr_opt\" -s large -l zh -p float32\n",
            "2024-10-24 12:39:54,722 - modelscope - INFO - PyTorch version 2.5.0 Found.\n",
            "2024-10-24 12:39:54,723 - modelscope - INFO - Loading ast index from /root/.cache/modelscope/ast_indexer\n",
            "2024-10-24 12:39:54,781 - modelscope - INFO - Loading done! Current index file version is 1.10.0, with md5 6f80e611660fd927919f53be0f3352ab and a total number of 946 components indexed\n",
            "2024-10-24 12:39:56,487 - modelscope - INFO - Use user-specified model revision: v2.0.4\n",
            "Downloading: 100% 10.9k/10.9k [00:00<00:00, 1.01MB/s]\n",
            "Downloading: 100% 173k/173k [00:00<00:00, 2.95MB/s]\n",
            "Downloading: 100% 2.45k/2.45k [00:00<00:00, 868kB/s]\n",
            "Downloading: 100% 472/472 [00:00<00:00, 174kB/s]\n",
            "Downloading: 100% 840M/840M [00:13<00:00, 66.5MB/s]\n",
            "Downloading: 100% 19.1k/19.1k [00:00<00:00, 1.26MB/s]\n",
            "Downloading: 100% 7.90M/7.90M [00:00<00:00, 38.4MB/s]\n",
            "Downloading: 100% 48.7k/48.7k [00:00<00:00, 1.36MB/s]\n",
            "Downloading: 100% 91.5k/91.5k [00:00<00:00, 1.87MB/s]\n",
            "ckpt: /root/.cache/modelscope/hub/iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/model.pt\n",
            "/usr/local/lib/python3.9/site-packages/funasr/train_utils/load_pretrained_model.py:68: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  src_state = torch.load(path, map_location=map_location)\n",
            "2024-10-24 12:40:29,506 - modelscope - INFO - Use user-specified model revision: v2.0.4\n",
            "Downloading: 100% 7.85k/7.85k [00:00<00:00, 2.90MB/s]\n",
            "Downloading: 100% 1.19k/1.19k [00:00<00:00, 435kB/s]\n",
            "Downloading: 100% 365/365 [00:00<00:00, 140kB/s]\n",
            "Downloading: 100% 1.64M/1.64M [00:00<00:00, 14.4MB/s]\n",
            "Downloading: 100% 8.45k/8.45k [00:00<00:00, 3.13MB/s]\n",
            "Downloading: 100% 27.3k/27.3k [00:00<00:00, 1.77MB/s]\n",
            "Downloading: 100% 2.16M/2.16M [00:00<00:00, 17.5MB/s]\n",
            "ckpt: /root/.cache/modelscope/hub/iic/speech_fsmn_vad_zh-cn-16k-common-pytorch/model.pt\n",
            "2024-10-24 12:40:38,683 - modelscope - INFO - Use user-specified model revision: v2.0.4\n",
            "Downloading: 100% 6.00k/6.00k [00:00<00:00, 2.14MB/s]\n",
            "Downloading: 100% 810/810 [00:00<00:00, 300kB/s]\n",
            "Downloading: 100% 373/373 [00:00<00:00, 134kB/s]\n",
            "Downloading: 100% 278M/278M [00:03<00:00, 85.9MB/s]\n",
            "Downloading: 100% 863/863 [00:00<00:00, 311kB/s]\n",
            "Downloading: 100% 11.2k/11.2k [00:00<00:00, 4.09MB/s]\n",
            "Downloading: 100% 151k/151k [00:00<00:00, 2.11MB/s]\n",
            "Downloading: 100% 4.01M/4.01M [00:00<00:00, 24.9MB/s]\n",
            "ckpt: /root/.cache/modelscope/hub/iic/punc_ct-transformer_zh-cn-common-vocab272727-pytorch/model.pt\n",
            "FunASR Ê®°ÂûãÂä†ËΩΩÂÆåÊàê: ZH\n",
            "  0% 0/6 [00:00<?, ?it/s]\n",
            "Êñá‰∏úË∑Ø 103.m4a_0000019200_0000238720.wav\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "100% 1/1 [00:00<00:00,  2.43it/s]\u001b[A\n",
            "{'load_data': '0.097', 'extract_feat': '0.121', 'forward': '0.411', 'batch_size': '1', 'rtf': '0.060'}, : 100% 1/1 [00:00<00:00,  2.43it/s]\u001b[A\n",
            "rtf_avg: 0.060: 100% 1/1 [00:00<00:00,  2.43it/s]\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A/usr/local/lib/python3.9/site-packages/funasr/models/paraformer/model.py:249: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(False):\n",
            "/usr/local/lib/python3.9/site-packages/funasr/models/paraformer/cif_predictor.py:212: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(False):\n",
            "\n",
            "\n",
            "100% 1/1 [00:01<00:00,  1.20s/it]\u001b[A\u001b[A\n",
            "\n",
            "{'load_data': '0.000', 'extract_feat': '0.011', 'forward': '1.198', 'batch_size': '1', 'rtf': '0.175'}, : 100% 1/1 [00:01<00:00,  1.20s/it]\u001b[A\u001b[A\n",
            "\n",
            "rtf_avg: 0.175: 100% 1/1 [00:01<00:00,  1.20s/it]\n",
            "\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "{'load_data': 0.0, 'extract_feat': 0.0, 'forward': '0.071', 'batch_size': '1', 'rtf': '-0.071'}, : 100% 1/1 [00:00<00:00, 14.05it/s]\u001b[A\u001b[A\n",
            "\n",
            "rtf_avg: -0.071: 100% 1/1 [00:00<00:00, 14.00it/s]\n",
            "\n",
            "100% 1/1 [00:01<00:00,  1.29s/it]\u001b[A\n",
            "rtf_avg: 0.186, time_speech:  6.860, time_escape: 1.275: 100% 1/1 [00:01<00:00,  1.29s/it]\n",
            " 17% 1/6 [00:01<00:08,  1.70s/it]\n",
            "Êñá‰∏úË∑Ø 103.m4a_0000238720_0000413280.wav\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "{'load_data': '0.010', 'extract_feat': '0.012', 'forward': '0.069', 'batch_size': '1', 'rtf': '0.013'}, : 100% 1/1 [00:00<00:00, 14.57it/s]\u001b[A\n",
            "rtf_avg: 0.013: 100% 1/1 [00:00<00:00, 14.43it/s]\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 1/1 [00:00<00:00,  9.08it/s]\u001b[A\u001b[A\n",
            "\n",
            "{'load_data': '0.000', 'extract_feat': '0.008', 'forward': '0.110', 'batch_size': '1', 'rtf': '0.020'}, : 100% 1/1 [00:00<00:00,  9.08it/s]\u001b[A\u001b[A\n",
            "\n",
            "rtf_avg: 0.020: 100% 1/1 [00:00<00:00,  9.03it/s]\n",
            "\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "{'load_data': 0.0, 'extract_feat': 0.0, 'forward': '0.012', 'batch_size': '1', 'rtf': '-0.012'}, : 100% 1/1 [00:00<00:00, 80.03it/s]\u001b[A\u001b[A\n",
            "\n",
            "rtf_avg: -0.012: 100% 1/1 [00:00<00:00, 78.24it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00,  7.21it/s]\u001b[A\n",
            "rtf_avg: 0.024, time_speech:  5.455, time_escape: 0.129: 100% 1/1 [00:00<00:00,  7.20it/s]\n",
            " 33% 2/6 [00:01<00:03,  1.22it/s]\n",
            "Êñá‰∏úË∑Ø 103.m4a_0000413280_0000599680.wav\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "{'load_data': '0.013', 'extract_feat': '0.012', 'forward': '0.075', 'batch_size': '1', 'rtf': '0.013'}, : 100% 1/1 [00:00<00:00, 13.32it/s]\u001b[A\n",
            "rtf_avg: 0.013: 100% 1/1 [00:00<00:00, 13.22it/s]\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 1/1 [00:00<00:00,  9.48it/s]\u001b[A\u001b[A\n",
            "\n",
            "{'load_data': '0.000', 'extract_feat': '0.008', 'forward': '0.105', 'batch_size': '1', 'rtf': '0.018'}, : 100% 1/1 [00:00<00:00,  9.48it/s]\u001b[A\u001b[A\n",
            "\n",
            "rtf_avg: 0.018: 100% 1/1 [00:00<00:00,  9.42it/s]\n",
            "\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "{'load_data': 0.0, 'extract_feat': 0.0, 'forward': '0.013', 'batch_size': '1', 'rtf': '-0.013'}, : 100% 1/1 [00:00<00:00, 74.64it/s]\u001b[A\u001b[A\n",
            "\n",
            "rtf_avg: -0.013: 100% 1/1 [00:00<00:00, 73.36it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00,  7.42it/s]\u001b[A\n",
            "rtf_avg: 0.021, time_speech:  5.825, time_escape: 0.125: 100% 1/1 [00:00<00:00,  7.41it/s]\n",
            " 50% 3/6 [00:02<00:01,  1.84it/s]\n",
            "Êñá‰∏úË∑Ø 103.m4a_0000599680_0000725280.wav\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "{'load_data': '0.010', 'extract_feat': '0.008', 'forward': '0.058', 'batch_size': '1', 'rtf': '0.015'}, : 100% 1/1 [00:00<00:00, 17.20it/s]\u001b[A\n",
            "rtf_avg: 0.015: 100% 1/1 [00:00<00:00, 17.06it/s]\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 1/1 [00:00<00:00,  9.53it/s]\u001b[A\u001b[A\n",
            "\n",
            "{'load_data': '0.000', 'extract_feat': '0.006', 'forward': '0.105', 'batch_size': '1', 'rtf': '0.027'}, : 100% 1/1 [00:00<00:00,  9.53it/s]\u001b[A\u001b[A\n",
            "\n",
            "rtf_avg: 0.027: 100% 1/1 [00:00<00:00,  9.47it/s]\n",
            "\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "{'load_data': 0.0, 'extract_feat': 0.0, 'forward': '0.006', 'batch_size': '1', 'rtf': '-0.006'}, : 100% 1/1 [00:00<00:00, 156.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "rtf_avg: -0.006: 100% 1/1 [00:00<00:00, 149.96it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00,  7.73it/s]\u001b[A\n",
            "rtf_avg: 0.030, time_speech:  3.925, time_escape: 0.117: 100% 1/1 [00:00<00:00,  7.71it/s]\n",
            " 67% 4/6 [00:02<00:00,  2.48it/s]\n",
            "Êñá‰∏úË∑Ø 103.m4a_0000725280_0000855200.wav\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "{'load_data': '0.010', 'extract_feat': '0.008', 'forward': '0.056', 'batch_size': '1', 'rtf': '0.014'}, : 100% 1/1 [00:00<00:00, 17.99it/s]\u001b[A\n",
            "rtf_avg: 0.014: 100% 1/1 [00:00<00:00, 17.90it/s]\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 1/1 [00:00<00:00,  9.98it/s]\u001b[A\u001b[A\n",
            "\n",
            "{'load_data': '0.000', 'extract_feat': '0.008', 'forward': '0.100', 'batch_size': '1', 'rtf': '0.025'}, : 100% 1/1 [00:00<00:00,  9.98it/s]\u001b[A\u001b[A\n",
            "\n",
            "rtf_avg: 0.025: 100% 1/1 [00:00<00:00,  9.92it/s]\n",
            "\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "{'load_data': 0.0, 'extract_feat': 0.0, 'forward': '0.006', 'batch_size': '1', 'rtf': '-0.006'}, : 100% 1/1 [00:00<00:00, 152.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "rtf_avg: -0.006: 100% 1/1 [00:00<00:00, 147.29it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00,  7.61it/s]\u001b[A\n",
            "rtf_avg: 0.028, time_speech:  4.060, time_escape: 0.116: 100% 1/1 [00:00<00:00,  7.59it/s]\n",
            " 83% 5/6 [00:02<00:00,  3.07it/s]\n",
            "Êñá‰∏úË∑Ø 103.m4a_0000855200_0000981120.wav\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "{'load_data': '0.010', 'extract_feat': '0.012', 'forward': '0.057', 'batch_size': '1', 'rtf': '0.014'}, : 100% 1/1 [00:00<00:00, 17.58it/s]\u001b[A\n",
            "rtf_avg: 0.014: 100% 1/1 [00:00<00:00, 17.50it/s]\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "{'load_data': '0.000', 'extract_feat': '0.006', 'forward': '0.099', 'batch_size': '1', 'rtf': '0.025'}, : 100% 1/1 [00:00<00:00, 10.14it/s]\u001b[A\u001b[A\n",
            "\n",
            "rtf_avg: 0.025: 100% 1/1 [00:00<00:00, 10.10it/s]\n",
            "\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "{'load_data': 0.0, 'extract_feat': 0.0, 'forward': '0.008', 'batch_size': '1', 'rtf': '-0.008'}, : 100% 1/1 [00:00<00:00, 126.27it/s]\u001b[A\u001b[A\n",
            "\n",
            "rtf_avg: -0.008: 100% 1/1 [00:00<00:00, 122.49it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00,  8.22it/s]\u001b[A\n",
            "rtf_avg: 0.028, time_speech:  3.935, time_escape: 0.112: 100% 1/1 [00:00<00:00,  8.21it/s]\n",
            "100% 6/6 [00:02<00:00,  2.24it/s]\n",
            "ASR ‰ªªÂä°ÂÆåÊàê->Ê†áÊ≥®Êñá‰ª∂Ë∑ØÂæÑ: /content/GPT-SoVITS/output/asr_opt/denoise_opt.list\n",
            "\n",
            "\"/usr/local/bin/python\" tools/subfix_webui.py --load_list \"/content/GPT-SoVITS/output/asr_opt/denoise_opt.list\" --webui_port 9871 --is_share True\n",
            "Running on local URL:  http://0.0.0.0:9871\n",
            "Running on public URL: https://766d61ee1ab2ae7d26.gradio.live\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/prepare_datasets/1-get-text.py\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/prepare_datasets/1-get-text.py\n",
            "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
            "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
            "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
            "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
            "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
            "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
            "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
            "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
            "Êñá‰∏úË∑Ø 103.m4a_0000238720_0000413280.wav\n",
            "Êñá‰∏úË∑Ø 103.m4a_0000019200_0000238720.wav\n",
            "ÂΩìÂâç‰ΩøÁî®g2pwËøõË°åÊãºÈü≥Êé®ÁêÜ\n",
            "ÂΩìÂâç‰ΩøÁî®g2pwËøõË°åÊãºÈü≥Êé®ÁêÜ\n",
            "Building prefix dict from the default dictionary ...\n",
            "Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /content/GPT-SoVITS/TEMP/jieba.cache\n",
            "Loading model cost 1.247 seconds.\n",
            "Prefix dict has been built succesfully.\n",
            "Dumping model to file cache /content/GPT-SoVITS/TEMP/jieba.cache\n",
            "Loading model cost 1.377 seconds.\n",
            "Prefix dict has been built succesfully.\n",
            "Êñá‰∏úË∑Ø 103.m4a_0000413280_0000599680.wav\n",
            "Êñá‰∏úË∑Ø 103.m4a_0000599680_0000725280.wav\n",
            "Êñá‰∏úË∑Ø 103.m4a_0000855200_0000981120.wav\n",
            "Êñá‰∏úË∑Ø 103.m4a_0000725280_0000855200.wav\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/prepare_datasets/2-get-hubert-wav32k.py\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/prepare_datasets/2-get-hubert-wav32k.py\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/prepare_datasets/3-get-semantic.py\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/prepare_datasets/3-get-semantic.py\n",
            "<All keys matched successfully>\n",
            "<All keys matched successfully>\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/s2_train.py --config \"/content/GPT-SoVITS/TEMP/tmp_s2.json\"\n",
            "phoneme_data_len: 6\n",
            "wav_data_len: 96\n",
            "100% 96/96 [00:00<00:00, 74703.74it/s]\n",
            "skipped_phone:  0 , skipped_dur:  0\n",
            "total left:  96\n",
            "<All keys matched successfully>\n",
            "<All keys matched successfully>\n",
            "  0% 0/12 [00:00<?, ?it/s][rank0]:[W1024 12:52:38.649308354 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
            "100% 12/12 [00:58<00:00,  4.86s/it]\n",
            "100% 12/12 [00:13<00:00,  1.11s/it]\n",
            "100% 12/12 [00:16<00:00,  1.36s/it]\n",
            "100% 12/12 [00:16<00:00,  1.38s/it]\n",
            "100% 12/12 [00:16<00:00,  1.40s/it]\n",
            "100% 12/12 [00:16<00:00,  1.35s/it]\n",
            "100% 12/12 [00:13<00:00,  1.13s/it]\n",
            "100% 12/12 [00:15<00:00,  1.26s/it]\n",
            "100% 12/12 [00:16<00:00,  1.39s/it]\n",
            "100% 12/12 [00:13<00:00,  1.12s/it]\n",
            "100% 12/12 [00:15<00:00,  1.27s/it]\n",
            "100% 12/12 [00:17<00:00,  1.44s/it]\n",
            "100% 12/12 [00:14<00:00,  1.18s/it]\n",
            "100% 12/12 [00:13<00:00,  1.15s/it]\n",
            "100% 12/12 [00:16<00:00,  1.37s/it]\n",
            "100% 12/12 [00:16<00:00,  1.38s/it]\n",
            "100% 12/12 [00:18<00:00,  1.52s/it]\n",
            "100% 12/12 [00:16<00:00,  1.37s/it]\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/s1_train.py --config_file \"/content/GPT-SoVITS/TEMP/tmp_s1.yaml\" \n",
            "Seed set to 1234\n",
            "Using 16bit Automatic Mixed Precision (AMP)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "/content/GPT-SoVITS/GPT_SoVITS/AR/models/t2s_lightning_module.py:26: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "<All keys matched successfully>\n",
            "ckpt_path: None\n",
            "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
            "----------------------------------------------------------------------------------------------------\n",
            "distributed_backend=nccl\n",
            "All distributed processes registered. Starting with 1 processes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "semantic_data_len: 6\n",
            "phoneme_data_len: 6\n",
            "                               item_name                                     semantic_audio\n",
            "0  Êñá‰∏úË∑Ø 103.m4a_0000019200_0000238720.wav  8 45 788 130 55 561 56 568 600 292 988 989 125...\n",
            "1  Êñá‰∏úË∑Ø 103.m4a_0000413280_0000599680.wav  457 775 729 59 284 610 331 688 495 430 765 842...\n",
            "2  Êñá‰∏úË∑Ø 103.m4a_0000725280_0000855200.wav  214 65 533 781 573 983 325 175 698 510 414 414...\n",
            "3  Êñá‰∏úË∑Ø 103.m4a_0000238720_0000413280.wav  612 239 196 695 378 678 678 268 761 972 826 12...\n",
            "4  Êñá‰∏úË∑Ø 103.m4a_0000599680_0000725280.wav  775 683 190 196 141 522 975 610 297 38 100 219...\n",
            "5  Êñá‰∏úË∑Ø 103.m4a_0000855200_0000981120.wav  634 748 504 634 504 515 1005 509 17 502 732 48...\n",
            "dataset.__len__(): 96\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name  | Type                 | Params | Mode \n",
            "-------------------------------------------------------\n",
            "0 | model | Text2SemanticDecoder | 77.6 M | train\n",
            "-------------------------------------------------------\n",
            "77.6 M    Trainable params\n",
            "0         Non-trainable params\n",
            "77.6 M    Total params\n",
            "310.426   Total estimated model params size (MB)\n",
            "257       Modules in train mode\n",
            "0         Modules in eval mode\n",
            "/usr/local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/content/GPT-SoVITS/GPT_SoVITS/AR/data/dataset.py:230: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  bert_feature = torch.load(path_bert, map_location=\"cpu\")\n",
            "/content/GPT-SoVITS/GPT_SoVITS/AR/data/dataset.py:230: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  bert_feature = torch.load(path_bert, map_location=\"cpu\")\n",
            "/content/GPT-SoVITS/GPT_SoVITS/AR/data/dataset.py:230: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  bert_feature = torch.load(path_bert, map_location=\"cpu\")\n",
            "/usr/local/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (12) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "/content/GPT-SoVITS/GPT_SoVITS/AR/data/dataset.py:230: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  bert_feature = torch.load(path_bert, map_location=\"cpu\")\n",
            "Epoch 17: 100% 12/12 [00:01<00:00,  6.76it/s, v_num=0, total_loss_step=623.0, lr_step=0.002, top_3_acc_step=0.999, total_loss_epoch=615.0, lr_epoch=0.002, top_3_acc_epoch=0.990]`Trainer.fit` stopped: `max_epochs=18` reached.\n",
            "Epoch 17: 100% 12/12 [00:01<00:00,  6.74it/s, v_num=0, total_loss_step=623.0, lr_step=0.002, top_3_acc_step=0.999, total_loss_epoch=615.0, lr_epoch=0.002, top_3_acc_epoch=0.990]\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/inference_webui_fast.py \"Auto\"\n",
            "---------------------------------------------TTS Config---------------------------------------------\n",
            "device              : cuda\n",
            "is_half             : True\n",
            "version             : v2\n",
            "t2s_weights_path    : GPT_weights_v2/Brian Pro Max-e15.ckpt\n",
            "vits_weights_path   : SoVITS_weights_v2/Brian Pro Max_e16_s192.pth\n",
            "bert_base_path      : GPT_SoVITS/pretrained_models/chinese-roberta-wwm-ext-large\n",
            "cnhuhbert_base_path : GPT_SoVITS/pretrained_models/chinese-hubert-base\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Loading Text2Semantic weights from GPT_weights_v2/Brian Pro Max-e15.ckpt\n",
            "Loading VITS weights from SoVITS_weights_v2/Brian Pro Max_e16_s192.pth\n",
            "Loading BERT weights from GPT_SoVITS/pretrained_models/chinese-roberta-wwm-ext-large\n",
            "Loading CNHuBERT weights from GPT_SoVITS/pretrained_models/chinese-hubert-base\n",
            "Running on local URL:  http://0.0.0.0:9872\n",
            "Running on public URL: https://da97b16789bf850ca3.gradio.live\n",
            "Set seed to 1293687262\n",
            "Âπ∂Ë°åÊé®ÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "ÂàÜÊ°∂Â§ÑÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "Actual Input Reference Text: Ê±üÂ§©‰∏ÄËâ≤Êó†Á∫§Â∞òÔºåÁöéÁöéÁ©∫‰∏≠Â≠§ÊúàËΩÆ„ÄÇ\n",
            "ÂΩìÂâç‰ΩøÁî®g2pwËøõË°åÊãºÈü≥Êé®ÁêÜ\n",
            "Building prefix dict from the default dictionary ...\n",
            "DEBUG:jieba_fast:Building prefix dict from the default dictionary ...\n",
            "Loading model from cache /content/GPT-SoVITS/TEMP/jieba.cache\n",
            "DEBUG:jieba_fast:Loading model from cache /content/GPT-SoVITS/TEMP/jieba.cache\n",
            "Loading model cost 1.363 seconds.\n",
            "DEBUG:jieba_fast:Loading model cost 1.363 seconds.\n",
            "Prefix dict has been built succesfully.\n",
            "DEBUG:jieba_fast:Prefix dict has been built succesfully.\n",
            "############ ÂàáÂàÜÊñáÊú¨ ############\n",
            "Actual Input Target Text:\n",
            "The past is the past and the present is the present.If you keep comparing youself to the past.Buddy,why don't you compare it when you were born?\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "[\"The past is the past and the present is the present.If you keep comparing youself to the past.Buddy,why don't you compare it when you were born?\"]\n",
            "############ ÊèêÂèñÊñáÊú¨BertÁâπÂæÅ ############\n",
            "  0% 0/1 [00:00<?, ?it/s][nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/cmudict.zip.\n",
            "  0% 0/1 [00:05<?, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/queueing.py\", line 522, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/route_utils.py\", line 260, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1689, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1267, in call_function\n",
            "    prediction = await utils.async_iteration(iterator)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 574, in async_iteration\n",
            "    return await iterator.__anext__()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 567, in __anext__\n",
            "    return await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 2441, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 943, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 550, in run_sync_iterator_async\n",
            "    return next(iterator)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 733, in gen_wrapper\n",
            "    response = next(iterator)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/inference_webui_fast.py\", line 151, in inference\n",
            "    for item in tts_pipeline.run(inputs):\n",
            "  File \"/usr/local/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 36, in generator_context\n",
            "    response = gen.send(None)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TTS.py\", line 774, in run\n",
            "    data = self.text_preprocessor.preprocess(text, text_lang, text_split_method, self.configs.version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 65, in preprocess\n",
            "    phones, bert_features, norm_text = self.segment_and_extract_feature_for_text(text, lang, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 117, in segment_and_extract_feature_for_text\n",
            "    return self.get_phones_and_bert(text, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 143, in get_phones_and_bert\n",
            "    phones, word2ph, norm_text = self.clean_text_inf(formattext, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 208, in clean_text_inf\n",
            "    phones, word2ph, norm_text = clean_text(text, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/cleaner.py\", line 46, in clean_text\n",
            "    phones = language_module.g2p(norm_text)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/english.py\", line 365, in g2p\n",
            "    phone_list = _g2p(text)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/english.py\", line 272, in __call__\n",
            "    tokens = pos_tag(words)  # tuples of (word, tag)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/__init__.py\", line 168, in pos_tag\n",
            "    tagger = _get_tagger(lang)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/__init__.py\", line 110, in _get_tagger\n",
            "    tagger = PerceptronTagger()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/perceptron.py\", line 183, in __init__\n",
            "    self.load_from_json(lang)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/perceptron.py\", line 273, in load_from_json\n",
            "    loc = find(f\"taggers/averaged_perceptron_tagger_{lang}/\")\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/data.py\", line 579, in find\n",
            "    raise LookupError(resource_not_found)\n",
            "LookupError: \n",
            "**********************************************************************\n",
            "  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n",
            "  Please use the NLTK Downloader to obtain the resource:\n",
            "\n",
            "  \u001b[31m>>> import nltk\n",
            "  >>> nltk.download('averaged_perceptron_tagger_eng')\n",
            "  \u001b[0m\n",
            "  For more information see: https://www.nltk.org/data.html\n",
            "\n",
            "  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n",
            "\n",
            "  Searched in:\n",
            "    - '/root/nltk_data'\n",
            "    - '/usr/local/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "**********************************************************************\n",
            "\n",
            "Set seed to 2872069240\n",
            "Âπ∂Ë°åÊé®ÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "ÂàÜÊ°∂Â§ÑÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "Actual Input Reference Text: Ê±üÂ§©‰∏ÄËâ≤Êó†Á∫§Â∞òÔºåÁöéÁöéÁ©∫‰∏≠Â≠§ÊúàËΩÆ„ÄÇ\n",
            "############ ÂàáÂàÜÊñáÊú¨ ############\n",
            "Actual Input Target Text:\n",
            "ËÆ©Êàë‰ª¨ËØ¥‰∏≠ÊñáÔºåÊ≤ôÂ®ÅÁéõÔºåÂì¶ÔºåÊ≤ôÂ®ÅÁéõ\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "['ËÆ©Êàë‰ª¨ËØ¥‰∏≠ÊñáÔºåÊ≤ôÂ®ÅÁéõÔºåÂì¶ÔºåÊ≤ôÂ®ÅÁéõ„ÄÇ']\n",
            "############ ÊèêÂèñÊñáÊú¨BertÁâπÂæÅ ############\n",
            "100% 1/1 [00:00<00:00,  7.43it/s]\n",
            "############ Êé®ÁêÜ ############\n",
            "Processed text from the frontend (per sentence): ['ËÆ©Êàë‰ª¨ËØ¥‰∏≠Êñá,Ê≤ôÂ®ÅÁéõ,Âì¶,Ê≤ôÂ®ÅÁéõ.']\n",
            "  8% 127/1500 [00:02<00:19, 69.26it/s]T2S Decoding EOS [131 -> 262]\n",
            "  9% 130/1500 [00:02<00:23, 57.87it/s]\n",
            "0.000\t0.136\t2.299\t0.619\n",
            "Set seed to 3687954205\n",
            "Âπ∂Ë°åÊé®ÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "ÂàÜÊ°∂Â§ÑÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "Actual Input Reference Text: Ê±üÂ§©‰∏ÄËâ≤Êó†Á∫§Â∞òÔºåÁöéÁöéÁ©∫‰∏≠Â≠§ÊúàËΩÆ„ÄÇ\n",
            "############ ÂàáÂàÜÊñáÊú¨ ############\n",
            "Actual Input Target Text:\n",
            "ËÆ©Êàë‰ª¨ËØ¥‰∏≠ÊñáÔºåÊ≤ôÂ®ÅÁéõÔºåÂì¶ÔºåÊ≤ôÂ®ÅÁéõ\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "['ËÆ©Êàë‰ª¨ËØ¥‰∏≠ÊñáÔºåÊ≤ôÂ®ÅÁéõÔºåÂì¶ÔºåÊ≤ôÂ®ÅÁéõ„ÄÇ']\n",
            "############ ÊèêÂèñÊñáÊú¨BertÁâπÂæÅ ############\n",
            "100% 1/1 [00:00<00:00, 18.43it/s]\n",
            "############ Êé®ÁêÜ ############\n",
            "Processed text from the frontend (per sentence): ['ËÆ©Êàë‰ª¨ËØ¥‰∏≠Êñá,Ê≤ôÂ®ÅÁéõ,Âì¶,Ê≤ôÂ®ÅÁéõ.']\n",
            "  1% 20/1500 [00:00<00:30, 48.60it/s]T2S Decoding EOS [131 -> 156]\n",
            "  2% 24/1500 [00:00<00:33, 44.71it/s]\n",
            "0.000\t0.055\t0.540\t0.318\n",
            "Set seed to 3587407266\n",
            "Âπ∂Ë°åÊé®ÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "ÂàÜÊ°∂Â§ÑÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "Actual Input Reference Text: Ê±üÂ§©‰∏ÄËâ≤Êó†Á∫§Â∞òÔºåÁöéÁöéÁ©∫‰∏≠Â≠§ÊúàËΩÆ„ÄÇ\n",
            "############ ÂàáÂàÜÊñáÊú¨ ############\n",
            "Actual Input Target Text:\n",
            "ËÆ©Êàë‰ª¨ËØ¥‰∏≠ÊñáÔºåÊ≤ôÂ®ÅÁéõÔºåÂì¶ÔºåÊ≤ôÂ®ÅÁéõ\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "['ËÆ©Êàë‰ª¨ËØ¥‰∏≠ÊñáÔºåÊ≤ôÂ®ÅÁéõÔºåÂì¶ÔºåÊ≤ôÂ®ÅÁéõ„ÄÇ']\n",
            "############ ÊèêÂèñÊñáÊú¨BertÁâπÂæÅ ############\n",
            "100% 1/1 [00:00<00:00, 1106.09it/s]\n",
            "############ Êé®ÁêÜ ############\n",
            "Processed text from the frontend (per sentence): ['ËÆ©Êàë‰ª¨ËØ¥‰∏≠Êñá,Ê≤ôÂ®ÅÁéõ,Âì¶,Ê≤ôÂ®ÅÁéõ.']\n",
            "  1% 15/1500 [00:00<00:20, 71.48it/s]T2S Decoding EOS [131 -> 154]\n",
            "  1% 22/1500 [00:00<00:22, 67.15it/s]\n",
            "0.000\t0.002\t0.330\t0.479\n",
            "Set seed to 3587407266\n",
            "Âπ∂Ë°åÊé®ÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "ÂàÜÊ°∂Â§ÑÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "Actual Input Reference Text: Ê±üÂ§©‰∏ÄËâ≤Êó†Á∫§Â∞òÔºåÁöéÁöéÁ©∫‰∏≠Â≠§ÊúàËΩÆ„ÄÇ\n",
            "############ ÂàáÂàÜÊñáÊú¨ ############\n",
            "Actual Input Target Text:\n",
            "ËÆ©Êàë‰ª¨ËØ¥‰∏≠ÊñáÔºåÊ≤ôÂ®ÅÁéõÔºåÂì¶ÔºåÊ≤ôÂ®ÅÁéõ\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "['ËÆ©Êàë‰ª¨ËØ¥‰∏≠ÊñáÔºåÊ≤ôÂ®ÅÁéõÔºåÂì¶ÔºåÊ≤ôÂ®ÅÁéõ„ÄÇ']\n",
            "############ ÊèêÂèñÊñáÊú¨BertÁâπÂæÅ ############\n",
            "100% 1/1 [00:00<00:00, 1105.22it/s]\n",
            "############ Êé®ÁêÜ ############\n",
            "Processed text from the frontend (per sentence): ['ËÆ©Êàë‰ª¨ËØ¥‰∏≠Êñá,Ê≤ôÂ®ÅÁéõ,Âì¶,Ê≤ôÂ®ÅÁéõ.']\n",
            "  1% 18/1500 [00:00<00:30, 48.67it/s]T2S Decoding EOS [131 -> 154]\n",
            "  1% 22/1500 [00:00<00:30, 48.35it/s]\n",
            "0.000\t0.002\t0.457\t0.078\n",
            "Set seed to 4081238147\n",
            "Âπ∂Ë°åÊé®ÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "ÂàÜÊ°∂Â§ÑÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "Actual Input Reference Text: Ê±üÂ§©‰∏ÄËâ≤Êó†Á∫§Â∞òÔºåÁöéÁöéÁ©∫‰∏≠Â≠§ÊúàËΩÆ„ÄÇ\n",
            "############ ÂàáÂàÜÊñáÊú¨ ############\n",
            "Actual Input Target Text:\n",
            "The past is the past\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "['The past is the past.']\n",
            "############ ÊèêÂèñÊñáÊú¨BertÁâπÂæÅ ############\n",
            "  0% 0/1 [00:00<?, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/queueing.py\", line 522, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/route_utils.py\", line 260, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1689, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1267, in call_function\n",
            "    prediction = await utils.async_iteration(iterator)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 574, in async_iteration\n",
            "    return await iterator.__anext__()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 567, in __anext__\n",
            "    return await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 2441, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 943, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 550, in run_sync_iterator_async\n",
            "    return next(iterator)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 733, in gen_wrapper\n",
            "    response = next(iterator)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/inference_webui_fast.py\", line 151, in inference\n",
            "    for item in tts_pipeline.run(inputs):\n",
            "  File \"/usr/local/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 36, in generator_context\n",
            "    response = gen.send(None)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TTS.py\", line 774, in run\n",
            "    data = self.text_preprocessor.preprocess(text, text_lang, text_split_method, self.configs.version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 65, in preprocess\n",
            "    phones, bert_features, norm_text = self.segment_and_extract_feature_for_text(text, lang, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 117, in segment_and_extract_feature_for_text\n",
            "    return self.get_phones_and_bert(text, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 143, in get_phones_and_bert\n",
            "    phones, word2ph, norm_text = self.clean_text_inf(formattext, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 208, in clean_text_inf\n",
            "    phones, word2ph, norm_text = clean_text(text, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/cleaner.py\", line 46, in clean_text\n",
            "    phones = language_module.g2p(norm_text)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/english.py\", line 365, in g2p\n",
            "    phone_list = _g2p(text)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/english.py\", line 272, in __call__\n",
            "    tokens = pos_tag(words)  # tuples of (word, tag)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/__init__.py\", line 168, in pos_tag\n",
            "    tagger = _get_tagger(lang)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/__init__.py\", line 110, in _get_tagger\n",
            "    tagger = PerceptronTagger()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/perceptron.py\", line 183, in __init__\n",
            "    self.load_from_json(lang)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/perceptron.py\", line 273, in load_from_json\n",
            "    loc = find(f\"taggers/averaged_perceptron_tagger_{lang}/\")\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/data.py\", line 579, in find\n",
            "    raise LookupError(resource_not_found)\n",
            "LookupError: \n",
            "**********************************************************************\n",
            "  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n",
            "  Please use the NLTK Downloader to obtain the resource:\n",
            "\n",
            "  \u001b[31m>>> import nltk\n",
            "  >>> nltk.download('averaged_perceptron_tagger_eng')\n",
            "  \u001b[0m\n",
            "  For more information see: https://www.nltk.org/data.html\n",
            "\n",
            "  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n",
            "\n",
            "  Searched in:\n",
            "    - '/root/nltk_data'\n",
            "    - '/usr/local/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "**********************************************************************\n",
            "\n",
            "Set seed to 2178643481\n",
            "Âπ∂Ë°åÊé®ÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "ÂàÜÊ°∂Â§ÑÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "Actual Input Reference Text: Ê±üÂ§©‰∏ÄËâ≤Êó†Á∫§Â∞òÔºåÁöéÁöéÁ©∫‰∏≠Â≠§ÊúàËΩÆ„ÄÇ\n",
            "############ ÂàáÂàÜÊñáÊú¨ ############\n",
            "Actual Input Target Text:\n",
            "The past is the past.\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "['The past is the past.']\n",
            "############ ÊèêÂèñÊñáÊú¨BertÁâπÂæÅ ############\n",
            "  0% 0/1 [00:00<?, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/queueing.py\", line 522, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/route_utils.py\", line 260, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1689, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1267, in call_function\n",
            "    prediction = await utils.async_iteration(iterator)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 574, in async_iteration\n",
            "    return await iterator.__anext__()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 567, in __anext__\n",
            "    return await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 2441, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 943, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 550, in run_sync_iterator_async\n",
            "    return next(iterator)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 733, in gen_wrapper\n",
            "    response = next(iterator)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/inference_webui_fast.py\", line 151, in inference\n",
            "    for item in tts_pipeline.run(inputs):\n",
            "  File \"/usr/local/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 36, in generator_context\n",
            "    response = gen.send(None)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TTS.py\", line 774, in run\n",
            "    data = self.text_preprocessor.preprocess(text, text_lang, text_split_method, self.configs.version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 65, in preprocess\n",
            "    phones, bert_features, norm_text = self.segment_and_extract_feature_for_text(text, lang, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 117, in segment_and_extract_feature_for_text\n",
            "    return self.get_phones_and_bert(text, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 143, in get_phones_and_bert\n",
            "    phones, word2ph, norm_text = self.clean_text_inf(formattext, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 208, in clean_text_inf\n",
            "    phones, word2ph, norm_text = clean_text(text, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/cleaner.py\", line 46, in clean_text\n",
            "    phones = language_module.g2p(norm_text)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/english.py\", line 365, in g2p\n",
            "    phone_list = _g2p(text)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/english.py\", line 272, in __call__\n",
            "    tokens = pos_tag(words)  # tuples of (word, tag)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/__init__.py\", line 168, in pos_tag\n",
            "    tagger = _get_tagger(lang)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/__init__.py\", line 110, in _get_tagger\n",
            "    tagger = PerceptronTagger()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/perceptron.py\", line 183, in __init__\n",
            "    self.load_from_json(lang)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/perceptron.py\", line 273, in load_from_json\n",
            "    loc = find(f\"taggers/averaged_perceptron_tagger_{lang}/\")\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/data.py\", line 579, in find\n",
            "    raise LookupError(resource_not_found)\n",
            "LookupError: \n",
            "**********************************************************************\n",
            "  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n",
            "  Please use the NLTK Downloader to obtain the resource:\n",
            "\n",
            "  \u001b[31m>>> import nltk\n",
            "  >>> nltk.download('averaged_perceptron_tagger_eng')\n",
            "  \u001b[0m\n",
            "  For more information see: https://www.nltk.org/data.html\n",
            "\n",
            "  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n",
            "\n",
            "  Searched in:\n",
            "    - '/root/nltk_data'\n",
            "    - '/usr/local/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "**********************************************************************\n",
            "\n",
            "Set seed to 571953045\n",
            "Âπ∂Ë°åÊé®ÁêÜÊ®°ÂºèÂ∑≤ÂÖ≥Èó≠\n",
            "ÂàÜÊ°∂Â§ÑÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "Actual Input Reference Text: Ê±üÂ§©‰∏ÄËâ≤Êó†Á∫§Â∞òÔºåÁöéÁöéÁ©∫‰∏≠Â≠§ÊúàËΩÆ„ÄÇ\n",
            "############ ÂàáÂàÜÊñáÊú¨ ############\n",
            "Actual Input Target Text:\n",
            "The past is the past.\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "['The past is the past.']\n",
            "############ ÊèêÂèñÊñáÊú¨BertÁâπÂæÅ ############\n",
            "  0% 0/1 [00:00<?, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/queueing.py\", line 522, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/route_utils.py\", line 260, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1689, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1267, in call_function\n",
            "    prediction = await utils.async_iteration(iterator)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 574, in async_iteration\n",
            "    return await iterator.__anext__()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 567, in __anext__\n",
            "    return await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 2441, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 943, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 550, in run_sync_iterator_async\n",
            "    return next(iterator)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 733, in gen_wrapper\n",
            "    response = next(iterator)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/inference_webui_fast.py\", line 151, in inference\n",
            "    for item in tts_pipeline.run(inputs):\n",
            "  File \"/usr/local/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 36, in generator_context\n",
            "    response = gen.send(None)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TTS.py\", line 774, in run\n",
            "    data = self.text_preprocessor.preprocess(text, text_lang, text_split_method, self.configs.version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 65, in preprocess\n",
            "    phones, bert_features, norm_text = self.segment_and_extract_feature_for_text(text, lang, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 117, in segment_and_extract_feature_for_text\n",
            "    return self.get_phones_and_bert(text, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 143, in get_phones_and_bert\n",
            "    phones, word2ph, norm_text = self.clean_text_inf(formattext, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 208, in clean_text_inf\n",
            "    phones, word2ph, norm_text = clean_text(text, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/cleaner.py\", line 46, in clean_text\n",
            "    phones = language_module.g2p(norm_text)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/english.py\", line 365, in g2p\n",
            "    phone_list = _g2p(text)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/english.py\", line 272, in __call__\n",
            "    tokens = pos_tag(words)  # tuples of (word, tag)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/__init__.py\", line 168, in pos_tag\n",
            "    tagger = _get_tagger(lang)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/__init__.py\", line 110, in _get_tagger\n",
            "    tagger = PerceptronTagger()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/perceptron.py\", line 183, in __init__\n",
            "    self.load_from_json(lang)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/perceptron.py\", line 273, in load_from_json\n",
            "    loc = find(f\"taggers/averaged_perceptron_tagger_{lang}/\")\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/data.py\", line 579, in find\n",
            "    raise LookupError(resource_not_found)\n",
            "LookupError: \n",
            "**********************************************************************\n",
            "  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n",
            "  Please use the NLTK Downloader to obtain the resource:\n",
            "\n",
            "  \u001b[31m>>> import nltk\n",
            "  >>> nltk.download('averaged_perceptron_tagger_eng')\n",
            "  \u001b[0m\n",
            "  For more information see: https://www.nltk.org/data.html\n",
            "\n",
            "  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n",
            "\n",
            "  Searched in:\n",
            "    - '/root/nltk_data'\n",
            "    - '/usr/local/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "**********************************************************************\n",
            "\n",
            "Set seed to 2094246127\n",
            "Âπ∂Ë°åÊé®ÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "ÂàÜÊ°∂Â§ÑÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "Actual Input Reference Text: Ê±üÂ§©‰∏ÄËâ≤Êó†Á∫§Â∞òÔºåÁöéÁöéÁ©∫‰∏≠Â≠§ÊúàËΩÆ„ÄÇ\n",
            "############ ÂàáÂàÜÊñáÊú¨ ############\n",
            "Actual Input Target Text:\n",
            "The past is the past.\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "['The past is the past.']\n",
            "############ ÊèêÂèñÊñáÊú¨BertÁâπÂæÅ ############\n",
            "  0% 0/1 [00:00<?, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/queueing.py\", line 522, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/route_utils.py\", line 260, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1689, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1267, in call_function\n",
            "    prediction = await utils.async_iteration(iterator)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 574, in async_iteration\n",
            "    return await iterator.__anext__()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 567, in __anext__\n",
            "    return await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 2441, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 943, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 550, in run_sync_iterator_async\n",
            "    return next(iterator)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 733, in gen_wrapper\n",
            "    response = next(iterator)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/inference_webui_fast.py\", line 151, in inference\n",
            "    for item in tts_pipeline.run(inputs):\n",
            "  File \"/usr/local/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 36, in generator_context\n",
            "    response = gen.send(None)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TTS.py\", line 774, in run\n",
            "    data = self.text_preprocessor.preprocess(text, text_lang, text_split_method, self.configs.version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 65, in preprocess\n",
            "    phones, bert_features, norm_text = self.segment_and_extract_feature_for_text(text, lang, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 117, in segment_and_extract_feature_for_text\n",
            "    return self.get_phones_and_bert(text, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 143, in get_phones_and_bert\n",
            "    phones, word2ph, norm_text = self.clean_text_inf(formattext, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 208, in clean_text_inf\n",
            "    phones, word2ph, norm_text = clean_text(text, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/cleaner.py\", line 46, in clean_text\n",
            "    phones = language_module.g2p(norm_text)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/english.py\", line 365, in g2p\n",
            "    phone_list = _g2p(text)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/english.py\", line 272, in __call__\n",
            "    tokens = pos_tag(words)  # tuples of (word, tag)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/__init__.py\", line 168, in pos_tag\n",
            "    tagger = _get_tagger(lang)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/__init__.py\", line 110, in _get_tagger\n",
            "    tagger = PerceptronTagger()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/perceptron.py\", line 183, in __init__\n",
            "    self.load_from_json(lang)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/perceptron.py\", line 273, in load_from_json\n",
            "    loc = find(f\"taggers/averaged_perceptron_tagger_{lang}/\")\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/data.py\", line 579, in find\n",
            "    raise LookupError(resource_not_found)\n",
            "LookupError: \n",
            "**********************************************************************\n",
            "  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n",
            "  Please use the NLTK Downloader to obtain the resource:\n",
            "\n",
            "  \u001b[31m>>> import nltk\n",
            "  >>> nltk.download('averaged_perceptron_tagger_eng')\n",
            "  \u001b[0m\n",
            "  For more information see: https://www.nltk.org/data.html\n",
            "\n",
            "  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n",
            "\n",
            "  Searched in:\n",
            "    - '/root/nltk_data'\n",
            "    - '/usr/local/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "**********************************************************************\n",
            "\n",
            "Set seed to 186337281\n",
            "Âπ∂Ë°åÊé®ÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "ÂàÜÊ°∂Â§ÑÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "Actual Input Reference Text: Ê±üÂ§©‰∏ÄËâ≤Êó†Á∫§Â∞òÔºåÁöéÁöéÁ©∫‰∏≠Â≠§ÊúàËΩÆ„ÄÇ\n",
            "############ ÂàáÂàÜÊñáÊú¨ ############\n",
            "Actual Input Target Text:\n",
            "The past is the past.\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "['The past is the past.']\n",
            "############ ÊèêÂèñÊñáÊú¨BertÁâπÂæÅ ############\n",
            "  0% 0/1 [00:00<?, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/queueing.py\", line 522, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/route_utils.py\", line 260, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1689, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1267, in call_function\n",
            "    prediction = await utils.async_iteration(iterator)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 574, in async_iteration\n",
            "    return await iterator.__anext__()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 567, in __anext__\n",
            "    return await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 2441, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 943, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 550, in run_sync_iterator_async\n",
            "    return next(iterator)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 733, in gen_wrapper\n",
            "    response = next(iterator)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/inference_webui_fast.py\", line 151, in inference\n",
            "    for item in tts_pipeline.run(inputs):\n",
            "  File \"/usr/local/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 36, in generator_context\n",
            "    response = gen.send(None)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TTS.py\", line 774, in run\n",
            "    data = self.text_preprocessor.preprocess(text, text_lang, text_split_method, self.configs.version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 65, in preprocess\n",
            "    phones, bert_features, norm_text = self.segment_and_extract_feature_for_text(text, lang, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 117, in segment_and_extract_feature_for_text\n",
            "    return self.get_phones_and_bert(text, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 143, in get_phones_and_bert\n",
            "    phones, word2ph, norm_text = self.clean_text_inf(formattext, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 208, in clean_text_inf\n",
            "    phones, word2ph, norm_text = clean_text(text, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/cleaner.py\", line 46, in clean_text\n",
            "    phones = language_module.g2p(norm_text)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/english.py\", line 365, in g2p\n",
            "    phone_list = _g2p(text)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/english.py\", line 272, in __call__\n",
            "    tokens = pos_tag(words)  # tuples of (word, tag)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/__init__.py\", line 168, in pos_tag\n",
            "    tagger = _get_tagger(lang)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/__init__.py\", line 110, in _get_tagger\n",
            "    tagger = PerceptronTagger()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/perceptron.py\", line 183, in __init__\n",
            "    self.load_from_json(lang)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/perceptron.py\", line 273, in load_from_json\n",
            "    loc = find(f\"taggers/averaged_perceptron_tagger_{lang}/\")\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/data.py\", line 579, in find\n",
            "    raise LookupError(resource_not_found)\n",
            "LookupError: \n",
            "**********************************************************************\n",
            "  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n",
            "  Please use the NLTK Downloader to obtain the resource:\n",
            "\n",
            "  \u001b[31m>>> import nltk\n",
            "  >>> nltk.download('averaged_perceptron_tagger_eng')\n",
            "  \u001b[0m\n",
            "  For more information see: https://www.nltk.org/data.html\n",
            "\n",
            "  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n",
            "\n",
            "  Searched in:\n",
            "    - '/root/nltk_data'\n",
            "    - '/usr/local/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "**********************************************************************\n",
            "\n",
            "Set seed to 3587407266\n",
            "Âπ∂Ë°åÊé®ÁêÜÊ®°ÂºèÂ∑≤ÂÖ≥Èó≠\n",
            "ÂàÜÊ°∂Â§ÑÁêÜÊ®°ÂºèÂ∑≤ÂÖ≥Èó≠\n",
            "Actual Input Reference Text: Ê±üÂ§©‰∏ÄËâ≤Êó†Á∫§Â∞òÔºåÁöéÁöéÁ©∫‰∏≠Â≠§ÊúàËΩÆ„ÄÇ\n",
            "############ ÂàáÂàÜÊñáÊú¨ ############\n",
            "Actual Input Target Text:\n",
            "The past is the past.\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "['The past is the past.']\n",
            "############ ÊèêÂèñÊñáÊú¨BertÁâπÂæÅ ############\n",
            "  0% 0/1 [00:00<?, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/queueing.py\", line 522, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/route_utils.py\", line 260, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1689, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1267, in call_function\n",
            "    prediction = await utils.async_iteration(iterator)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 574, in async_iteration\n",
            "    return await iterator.__anext__()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 567, in __anext__\n",
            "    return await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 2441, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 943, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 550, in run_sync_iterator_async\n",
            "    return next(iterator)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 733, in gen_wrapper\n",
            "    response = next(iterator)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/inference_webui_fast.py\", line 151, in inference\n",
            "    for item in tts_pipeline.run(inputs):\n",
            "  File \"/usr/local/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 36, in generator_context\n",
            "    response = gen.send(None)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TTS.py\", line 774, in run\n",
            "    data = self.text_preprocessor.preprocess(text, text_lang, text_split_method, self.configs.version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 65, in preprocess\n",
            "    phones, bert_features, norm_text = self.segment_and_extract_feature_for_text(text, lang, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 117, in segment_and_extract_feature_for_text\n",
            "    return self.get_phones_and_bert(text, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 143, in get_phones_and_bert\n",
            "    phones, word2ph, norm_text = self.clean_text_inf(formattext, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 208, in clean_text_inf\n",
            "    phones, word2ph, norm_text = clean_text(text, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/cleaner.py\", line 46, in clean_text\n",
            "    phones = language_module.g2p(norm_text)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/english.py\", line 365, in g2p\n",
            "    phone_list = _g2p(text)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/english.py\", line 272, in __call__\n",
            "    tokens = pos_tag(words)  # tuples of (word, tag)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/__init__.py\", line 168, in pos_tag\n",
            "    tagger = _get_tagger(lang)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/__init__.py\", line 110, in _get_tagger\n",
            "    tagger = PerceptronTagger()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/perceptron.py\", line 183, in __init__\n",
            "    self.load_from_json(lang)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/perceptron.py\", line 273, in load_from_json\n",
            "    loc = find(f\"taggers/averaged_perceptron_tagger_{lang}/\")\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/data.py\", line 579, in find\n",
            "    raise LookupError(resource_not_found)\n",
            "LookupError: \n",
            "**********************************************************************\n",
            "  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n",
            "  Please use the NLTK Downloader to obtain the resource:\n",
            "\n",
            "  \u001b[31m>>> import nltk\n",
            "  >>> nltk.download('averaged_perceptron_tagger_eng')\n",
            "  \u001b[0m\n",
            "  For more information see: https://www.nltk.org/data.html\n",
            "\n",
            "  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n",
            "\n",
            "  Searched in:\n",
            "    - '/root/nltk_data'\n",
            "    - '/usr/local/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "**********************************************************************\n",
            "\n",
            "Set seed to 4081238147\n",
            "Âπ∂Ë°åÊé®ÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "ÂàÜÊ°∂Â§ÑÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "Actual Input Reference Text: Ê±üÂ§©‰∏ÄËâ≤Êó†Á∫§Â∞òÔºåÁöéÁöéÁ©∫‰∏≠Â≠§ÊúàËΩÆ„ÄÇ\n",
            "############ ÂàáÂàÜÊñáÊú¨ ############\n",
            "Actual Input Target Text:\n",
            "The past is the past.\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "['The past is the past.']\n",
            "############ ÊèêÂèñÊñáÊú¨BertÁâπÂæÅ ############\n",
            "  0% 0/1 [00:00<?, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/queueing.py\", line 522, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/route_utils.py\", line 260, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1689, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1267, in call_function\n",
            "    prediction = await utils.async_iteration(iterator)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 574, in async_iteration\n",
            "    return await iterator.__anext__()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 567, in __anext__\n",
            "    return await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 2441, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 943, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 550, in run_sync_iterator_async\n",
            "    return next(iterator)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 733, in gen_wrapper\n",
            "    response = next(iterator)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/inference_webui_fast.py\", line 151, in inference\n",
            "    for item in tts_pipeline.run(inputs):\n",
            "  File \"/usr/local/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 36, in generator_context\n",
            "    response = gen.send(None)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TTS.py\", line 774, in run\n",
            "    data = self.text_preprocessor.preprocess(text, text_lang, text_split_method, self.configs.version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 65, in preprocess\n",
            "    phones, bert_features, norm_text = self.segment_and_extract_feature_for_text(text, lang, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 117, in segment_and_extract_feature_for_text\n",
            "    return self.get_phones_and_bert(text, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 143, in get_phones_and_bert\n",
            "    phones, word2ph, norm_text = self.clean_text_inf(formattext, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 208, in clean_text_inf\n",
            "    phones, word2ph, norm_text = clean_text(text, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/cleaner.py\", line 46, in clean_text\n",
            "    phones = language_module.g2p(norm_text)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/english.py\", line 365, in g2p\n",
            "    phone_list = _g2p(text)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/english.py\", line 272, in __call__\n",
            "    tokens = pos_tag(words)  # tuples of (word, tag)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/__init__.py\", line 168, in pos_tag\n",
            "    tagger = _get_tagger(lang)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/__init__.py\", line 110, in _get_tagger\n",
            "    tagger = PerceptronTagger()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/perceptron.py\", line 183, in __init__\n",
            "    self.load_from_json(lang)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/perceptron.py\", line 273, in load_from_json\n",
            "    loc = find(f\"taggers/averaged_perceptron_tagger_{lang}/\")\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/data.py\", line 579, in find\n",
            "    raise LookupError(resource_not_found)\n",
            "LookupError: \n",
            "**********************************************************************\n",
            "  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n",
            "  Please use the NLTK Downloader to obtain the resource:\n",
            "\n",
            "  \u001b[31m>>> import nltk\n",
            "  >>> nltk.download('averaged_perceptron_tagger_eng')\n",
            "  \u001b[0m\n",
            "  For more information see: https://www.nltk.org/data.html\n",
            "\n",
            "  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n",
            "\n",
            "  Searched in:\n",
            "    - '/root/nltk_data'\n",
            "    - '/usr/local/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "**********************************************************************\n",
            "\n",
            "Set seed to 2178643481\n",
            "Âπ∂Ë°åÊé®ÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "ÂàÜÊ°∂Â§ÑÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Actual Input Reference Text: The past is the past and the present is the present.If you keep comparing yourself to the past.Buddy,why don't you compare it when you were born.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/queueing.py\", line 522, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/route_utils.py\", line 260, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1689, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1267, in call_function\n",
            "    prediction = await utils.async_iteration(iterator)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 574, in async_iteration\n",
            "    return await iterator.__anext__()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 567, in __anext__\n",
            "    return await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 2441, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 943, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 550, in run_sync_iterator_async\n",
            "    return next(iterator)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 733, in gen_wrapper\n",
            "    response = next(iterator)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/inference_webui_fast.py\", line 151, in inference\n",
            "    for item in tts_pipeline.run(inputs):\n",
            "  File \"/usr/local/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 36, in generator_context\n",
            "    response = gen.send(None)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TTS.py\", line 759, in run\n",
            "    self.text_preprocessor.segment_and_extract_feature_for_text(\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 117, in segment_and_extract_feature_for_text\n",
            "    return self.get_phones_and_bert(text, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 143, in get_phones_and_bert\n",
            "    phones, word2ph, norm_text = self.clean_text_inf(formattext, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 208, in clean_text_inf\n",
            "    phones, word2ph, norm_text = clean_text(text, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/cleaner.py\", line 46, in clean_text\n",
            "    phones = language_module.g2p(norm_text)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/english.py\", line 365, in g2p\n",
            "    phone_list = _g2p(text)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/english.py\", line 272, in __call__\n",
            "    tokens = pos_tag(words)  # tuples of (word, tag)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/__init__.py\", line 168, in pos_tag\n",
            "    tagger = _get_tagger(lang)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/__init__.py\", line 110, in _get_tagger\n",
            "    tagger = PerceptronTagger()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/perceptron.py\", line 183, in __init__\n",
            "    self.load_from_json(lang)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/perceptron.py\", line 273, in load_from_json\n",
            "    loc = find(f\"taggers/averaged_perceptron_tagger_{lang}/\")\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/data.py\", line 579, in find\n",
            "    raise LookupError(resource_not_found)\n",
            "LookupError: \n",
            "**********************************************************************\n",
            "  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n",
            "  Please use the NLTK Downloader to obtain the resource:\n",
            "\n",
            "  \u001b[31m>>> import nltk\n",
            "  >>> nltk.download('averaged_perceptron_tagger_eng')\n",
            "  \u001b[0m\n",
            "  For more information see: https://www.nltk.org/data.html\n",
            "\n",
            "  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n",
            "\n",
            "  Searched in:\n",
            "    - '/root/nltk_data'\n",
            "    - '/usr/local/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "**********************************************************************\n",
            "\n",
            "Set seed to 571953045\n",
            "Âπ∂Ë°åÊé®ÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "ÂàÜÊ°∂Â§ÑÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "Actual Input Reference Text: The past is the past and the present is the present.If you keep comparing yourself to the past.Buddy,why don't you compare it when you were born.\n",
            "############ ÂàáÂàÜÊñáÊú¨ ############\n",
            "Actual Input Target Text:\n",
            "The past is the past.\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "['The past is the past.']\n",
            "############ ÊèêÂèñÊñáÊú¨BertÁâπÂæÅ ############\n",
            "  0% 0/1 [00:00<?, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/queueing.py\", line 522, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/route_utils.py\", line 260, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1689, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1267, in call_function\n",
            "    prediction = await utils.async_iteration(iterator)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 574, in async_iteration\n",
            "    return await iterator.__anext__()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 567, in __anext__\n",
            "    return await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 2441, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 943, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 550, in run_sync_iterator_async\n",
            "    return next(iterator)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 733, in gen_wrapper\n",
            "    response = next(iterator)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/inference_webui_fast.py\", line 151, in inference\n",
            "    for item in tts_pipeline.run(inputs):\n",
            "  File \"/usr/local/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 36, in generator_context\n",
            "    response = gen.send(None)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TTS.py\", line 774, in run\n",
            "    data = self.text_preprocessor.preprocess(text, text_lang, text_split_method, self.configs.version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 65, in preprocess\n",
            "    phones, bert_features, norm_text = self.segment_and_extract_feature_for_text(text, lang, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 117, in segment_and_extract_feature_for_text\n",
            "    return self.get_phones_and_bert(text, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 143, in get_phones_and_bert\n",
            "    phones, word2ph, norm_text = self.clean_text_inf(formattext, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 208, in clean_text_inf\n",
            "    phones, word2ph, norm_text = clean_text(text, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/cleaner.py\", line 46, in clean_text\n",
            "    phones = language_module.g2p(norm_text)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/english.py\", line 365, in g2p\n",
            "    phone_list = _g2p(text)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/english.py\", line 272, in __call__\n",
            "    tokens = pos_tag(words)  # tuples of (word, tag)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/__init__.py\", line 168, in pos_tag\n",
            "    tagger = _get_tagger(lang)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/__init__.py\", line 110, in _get_tagger\n",
            "    tagger = PerceptronTagger()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/perceptron.py\", line 183, in __init__\n",
            "    self.load_from_json(lang)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/perceptron.py\", line 273, in load_from_json\n",
            "    loc = find(f\"taggers/averaged_perceptron_tagger_{lang}/\")\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/data.py\", line 579, in find\n",
            "    raise LookupError(resource_not_found)\n",
            "LookupError: \n",
            "**********************************************************************\n",
            "  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n",
            "  Please use the NLTK Downloader to obtain the resource:\n",
            "\n",
            "  \u001b[31m>>> import nltk\n",
            "  >>> nltk.download('averaged_perceptron_tagger_eng')\n",
            "  \u001b[0m\n",
            "  For more information see: https://www.nltk.org/data.html\n",
            "\n",
            "  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n",
            "\n",
            "  Searched in:\n",
            "    - '/root/nltk_data'\n",
            "    - '/usr/local/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "**********************************************************************\n",
            "\n",
            "Set seed to 2094246127\n",
            "Âπ∂Ë°åÊé®ÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "ÂàÜÊ°∂Â§ÑÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "Actual Input Reference Text: The past is the past and the present is the present.If you keep comparing yourself to the past.Buddy,why don't you compare it when you were born.\n",
            "############ ÂàáÂàÜÊñáÊú¨ ############\n",
            "Actual Input Target Text:\n",
            "The past is the past.\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "['The past is the past.']\n",
            "############ ÊèêÂèñÊñáÊú¨BertÁâπÂæÅ ############\n",
            "  0% 0/1 [00:00<?, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/queueing.py\", line 522, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/route_utils.py\", line 260, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1689, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1267, in call_function\n",
            "    prediction = await utils.async_iteration(iterator)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 574, in async_iteration\n",
            "    return await iterator.__anext__()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 567, in __anext__\n",
            "    return await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 2441, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 943, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 550, in run_sync_iterator_async\n",
            "    return next(iterator)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 733, in gen_wrapper\n",
            "    response = next(iterator)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/inference_webui_fast.py\", line 151, in inference\n",
            "    for item in tts_pipeline.run(inputs):\n",
            "  File \"/usr/local/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 36, in generator_context\n",
            "    response = gen.send(None)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TTS.py\", line 774, in run\n",
            "    data = self.text_preprocessor.preprocess(text, text_lang, text_split_method, self.configs.version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 65, in preprocess\n",
            "    phones, bert_features, norm_text = self.segment_and_extract_feature_for_text(text, lang, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 117, in segment_and_extract_feature_for_text\n",
            "    return self.get_phones_and_bert(text, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 143, in get_phones_and_bert\n",
            "    phones, word2ph, norm_text = self.clean_text_inf(formattext, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 208, in clean_text_inf\n",
            "    phones, word2ph, norm_text = clean_text(text, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/cleaner.py\", line 46, in clean_text\n",
            "    phones = language_module.g2p(norm_text)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/english.py\", line 365, in g2p\n",
            "    phone_list = _g2p(text)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/english.py\", line 272, in __call__\n",
            "    tokens = pos_tag(words)  # tuples of (word, tag)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/__init__.py\", line 168, in pos_tag\n",
            "    tagger = _get_tagger(lang)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/__init__.py\", line 110, in _get_tagger\n",
            "    tagger = PerceptronTagger()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/perceptron.py\", line 183, in __init__\n",
            "    self.load_from_json(lang)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/perceptron.py\", line 273, in load_from_json\n",
            "    loc = find(f\"taggers/averaged_perceptron_tagger_{lang}/\")\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/data.py\", line 579, in find\n",
            "    raise LookupError(resource_not_found)\n",
            "LookupError: \n",
            "**********************************************************************\n",
            "  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n",
            "  Please use the NLTK Downloader to obtain the resource:\n",
            "\n",
            "  \u001b[31m>>> import nltk\n",
            "  >>> nltk.download('averaged_perceptron_tagger_eng')\n",
            "  \u001b[0m\n",
            "  For more information see: https://www.nltk.org/data.html\n",
            "\n",
            "  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n",
            "\n",
            "  Searched in:\n",
            "    - '/root/nltk_data'\n",
            "    - '/usr/local/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "**********************************************************************\n",
            "\n",
            "Set seed to 186337281\n",
            "Âπ∂Ë°åÊé®ÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "ÂàÜÊ°∂Â§ÑÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "############ ÂàáÂàÜÊñáÊú¨ ############\n",
            "Actual Input Target Text:\n",
            "The past is the past.\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "['The past is the past.']\n",
            "############ ÊèêÂèñÊñáÊú¨BertÁâπÂæÅ ############\n",
            "  0% 0/1 [00:00<?, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/queueing.py\", line 522, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/route_utils.py\", line 260, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1689, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1267, in call_function\n",
            "    prediction = await utils.async_iteration(iterator)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 574, in async_iteration\n",
            "    return await iterator.__anext__()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 567, in __anext__\n",
            "    return await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 2441, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 943, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 550, in run_sync_iterator_async\n",
            "    return next(iterator)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 733, in gen_wrapper\n",
            "    response = next(iterator)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/inference_webui_fast.py\", line 151, in inference\n",
            "    for item in tts_pipeline.run(inputs):\n",
            "  File \"/usr/local/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 36, in generator_context\n",
            "    response = gen.send(None)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TTS.py\", line 774, in run\n",
            "    data = self.text_preprocessor.preprocess(text, text_lang, text_split_method, self.configs.version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 65, in preprocess\n",
            "    phones, bert_features, norm_text = self.segment_and_extract_feature_for_text(text, lang, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 117, in segment_and_extract_feature_for_text\n",
            "    return self.get_phones_and_bert(text, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 143, in get_phones_and_bert\n",
            "    phones, word2ph, norm_text = self.clean_text_inf(formattext, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 208, in clean_text_inf\n",
            "    phones, word2ph, norm_text = clean_text(text, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/cleaner.py\", line 46, in clean_text\n",
            "    phones = language_module.g2p(norm_text)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/english.py\", line 365, in g2p\n",
            "    phone_list = _g2p(text)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/english.py\", line 272, in __call__\n",
            "    tokens = pos_tag(words)  # tuples of (word, tag)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/__init__.py\", line 168, in pos_tag\n",
            "    tagger = _get_tagger(lang)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/__init__.py\", line 110, in _get_tagger\n",
            "    tagger = PerceptronTagger()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/perceptron.py\", line 183, in __init__\n",
            "    self.load_from_json(lang)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/perceptron.py\", line 273, in load_from_json\n",
            "    loc = find(f\"taggers/averaged_perceptron_tagger_{lang}/\")\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/data.py\", line 579, in find\n",
            "    raise LookupError(resource_not_found)\n",
            "LookupError: \n",
            "**********************************************************************\n",
            "  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n",
            "  Please use the NLTK Downloader to obtain the resource:\n",
            "\n",
            "  \u001b[31m>>> import nltk\n",
            "  >>> nltk.download('averaged_perceptron_tagger_eng')\n",
            "  \u001b[0m\n",
            "  For more information see: https://www.nltk.org/data.html\n",
            "\n",
            "  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n",
            "\n",
            "  Searched in:\n",
            "    - '/root/nltk_data'\n",
            "    - '/usr/local/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "**********************************************************************\n",
            "\n"
          ]
        }
      ]
    }
  ]
}